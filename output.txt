
--- ./package-lock.json ---
{
  "name": "abl",
  "version": "2.0.0-alpha.1",
  "lockfileVersion": 3,
  "requires": true,
  "packages": {
    "": {
      "name": "abl",
      "version": "2.0.0-alpha.1",
      "license": "MIT",
      "dependencies": {
        "chalk": "^4.1.2",
        "commander": "^12.0.0",
        "js-yaml": "^4.1.0"
      },
      "bin": {
        "abl": "src/index.js"
      },
      "engines": {
        "node": ">=18.0.0"
      }
    },
    "node_modules/ansi-styles": {
      "version": "4.3.0",
      "resolved": "https://registry.npmjs.org/ansi-styles/-/ansi-styles-4.3.0.tgz",
      "integrity": "sha512-zbB9rCJAT1rbjiVDb2hqKFHNYLxgtk8NURxZ3IZwD3F6NtxbXZQCnnSi1Lkx+IDohdPlFp222wVALIheZJQSEg==",
      "license": "MIT",
      "dependencies": {
        "color-convert": "^2.0.1"
      },
      "engines": {
        "node": ">=8"
      },
      "funding": {
        "url": "https://github.com/chalk/ansi-styles?sponsor=1"
      }
    },
    "node_modules/argparse": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/argparse/-/argparse-2.0.1.tgz",
      "integrity": "sha512-8+9WqebbFzpX9OR+Wa6O29asIogeRMzcGtAINdpMHHyAg10f05aSFVBbcEqGf/PXw1EjAZ+q2/bEBg3DvurK3Q==",
      "license": "Python-2.0"
    },
    "node_modules/chalk": {
      "version": "4.1.2",
      "resolved": "https://registry.npmjs.org/chalk/-/chalk-4.1.2.tgz",
      "integrity": "sha512-oKnbhFyRIXpUuez8iBMmyEa4nbj4IOQyuhc/wy9kY7/WVPcwIO9VA668Pu8RkO7+0G76SLROeyw9CpQ061i4mA==",
      "license": "MIT",
      "dependencies": {
        "ansi-styles": "^4.1.0",
        "supports-color": "^7.1.0"
      },
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/chalk/chalk?sponsor=1"
      }
    },
    "node_modules/color-convert": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/color-convert/-/color-convert-2.0.1.tgz",
      "integrity": "sha512-RRECPsj7iu/xb5oKYcsFHSppFNnsj/52OVTRKb4zP5onXwVF3zVmmToNcOfGC+CRDpfK/U584fMg38ZHCaElKQ==",
      "license": "MIT",
      "dependencies": {
        "color-name": "~1.1.4"
      },
      "engines": {
        "node": ">=7.0.0"
      }
    },
    "node_modules/color-name": {
      "version": "1.1.4",
      "resolved": "https://registry.npmjs.org/color-name/-/color-name-1.1.4.tgz",
      "integrity": "sha512-dOy+3AuW3a2wNbZHIuMZpTcgjGuLU/uBL/ubcZF9OXbDo8ff4O8yVp5Bf0efS8uEoYo5q4Fx7dY9OgQGXgAsQA==",
      "license": "MIT"
    },
    "node_modules/commander": {
      "version": "12.1.0",
      "resolved": "https://registry.npmjs.org/commander/-/commander-12.1.0.tgz",
      "integrity": "sha512-Vw8qHK3bZM9y/P10u3Vib8o/DdkvA2OtPtZvD871QKjy74Wj1WSKFILMPRPSdUSx5RFK1arlJzEtA4PkFgnbuA==",
      "license": "MIT",
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/has-flag": {
      "version": "4.0.0",
      "resolved": "https://registry.npmjs.org/has-flag/-/has-flag-4.0.0.tgz",
      "integrity": "sha512-EykJT/Q1KjTWctppgIAgfSO0tKVuZUjhgMr17kqTumMl6Afv3EISleU7qZUzoXDFTAHTDC4NOoG/ZxU3EvlMPQ==",
      "license": "MIT",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/js-yaml": {
      "version": "4.1.1",
      "resolved": "https://registry.npmjs.org/js-yaml/-/js-yaml-4.1.1.tgz",
      "integrity": "sha512-qQKT4zQxXl8lLwBtHMWwaTcGfFOZviOJet3Oy/xmGk2gZH677CJM9EvtfdSkgWcATZhj/55JZ0rmy3myCT5lsA==",
      "license": "MIT",
      "dependencies": {
        "argparse": "^2.0.1"
      },
      "bin": {
        "js-yaml": "bin/js-yaml.js"
      }
    },
    "node_modules/supports-color": {
      "version": "7.2.0",
      "resolved": "https://registry.npmjs.org/supports-color/-/supports-color-7.2.0.tgz",
      "integrity": "sha512-qpCAvRl9stuOHveKsn7HncJRvv501qIacKzQlO/+Lwxc9+0q2wLyv4Dfvt80/DPn2pqOBsJdDiogXGR9+OvwRw==",
      "license": "MIT",
      "dependencies": {
        "has-flag": "^4.0.0"
      },
      "engines": {
        "node": ">=8"
      }
    }
  }
}

--- ./prompts/verifier.md ---
# Verifier

You are an adversarial QA engineer. You are currently testing the implementation of the **Phase** and **Iteration** specified in the "Session State" section of your context. Your job is to prove the implementation is wrong, not to confirm it is right. You test the live running system against the specs. You never read source code.

## Your workspace
- You are executed in the project's tests directory. This is your root.
- You can read and write files freely here.
- The "Current Phase Spec" is in your context.
- To access specs from previous phases (for cumulative testing), use: `abl-cmd get-spec <N>`

## Your context (injected)
- Project description and map
- Session State (Phase/Iteration)
- Current Phase Spec
- Available Commands

## How to work
1. Read the "Current Phase Spec" to understand the scope.
2. If this is a cumulative phase, use `abl-cmd get-spec <N>` to retrieve previous contracts as needed.
3. Check your working directory for existing test scripts — reuse and refine them.
4. **Prepare Environment:** Before running tests, you must ensure the system is in a clean and ready state. Use the tools provided in "Available Commands" via `abl-cmd` (e.g. seeding).
5. Run tests using bash — curl, playwright, or any appropriate tool.
6. Collect real stdout/stderr — never assume or hallucinate results.
7. Be adversarial — explicitly test Happy Paths, Bad Paths, and Edge Cases.
8. Session isolation: each independent test scenario must establish its own fresh session.

## Your sole output signal is failed_specs.md

This file is the only thing that determines pass or fail.

- If ALL contracts pass: delete `failed_specs.md` if it exists.
- If ANY contract fails: write `failed_specs.md` with every failure in the specified format.

Do not report pass or fail in prose. The file presence is the verdict.

## failed_specs.md format

```
SPEC: [exact contract that failed]
INPUT: [exact input/payload/request used]
OBSERVED: [exact response received]

SPEC: [next failure]
INPUT: ...
OBSERVED: ...
```

## Failed Specs File Management
If you run multiple test scripts, aggregate all results into a single
`failed_specs.md` at the end — do not rely on individual script outputs.

## Run Report (mandatory — write this before closing)

Write `verifier_reports/phase{N}_v{I}_{YYYYMMDD_HHMM}.md` in your current directory before closing. Always — pass or fail. Replace `{N}` and `{I}` with the Phase and Iteration numbers from your Session State.

```markdown
# Verifier Run Report
Phase: {N} | Iteration: {I} | {timestamp}

## What I did
- Which specs I reviewed
- Which test files already existed vs what I created fresh
- How many test scripts I ran and in what order
- Total contracts tested / passed / failed

## Friction log
- Every moment you had to probe, retry, or guess to understand system behavior
- Any spec ambiguities that forced assumptions
- Any context that was missing and required extra tool calls to discover
- Any endpoint behaviors that surprised you or required multiple attempts

## Turn self-assessment
- Rough split: how many turns were productive (writing/running tests) vs discovery (figuring out what exists, port, auth flow, response shape)
- What single piece of information, if provided upfront, would have saved the most turns this run?
```

## Rules
- Never read source files — you are a black box tester.
- Never write or modify application code.
- **You are responsible for preparing the environment via `abl-cmd` before testing.**
- Every test must be anchored to a spec contract — no invented expectations.
- **You MUST write the Verifier Run Report before finishing.**
- Your task is complete when failed_specs.md is written or deleted AND run report is written.
--- ./prompts/builder.md ---
# Builder

You are a senior engineer. You are currently working on the **Phase** and **Iteration** specified in the "Session State" section of your context. Your job is to implement the current phase spec exactly. Nothing more.

## Your workspace
- You are executed in the project's source directory. This is your root.
- You can read and write files freely here.
- The spec for the current phase is provided in the "Current Phase Spec" section of your context.

## Your context (injected)
- Project description and map
- Session State (Phase/Iteration)
- Current Phase Spec
- Available Commands

## Shipping Standards: Zero Tolerance
You are responsible for the technical integrity of the code you ship. A phase is not complete until the implementation is stable and professional.
1. **Zero Errors:** Your code must ship with no runtime errors, no lint errors, and no type errors.
2. **Health Checks:** If the "Available Commands" section includes a health check or validation command, you MUST run it via `abl-cmd` and ensure it passes before finishing your turn.
3. **Significant Warnings:** The code must run without significant warnings. Do not leave "todo" comments or half-finished implementations.
4. **Environment Management:** Use the provided commands as necessary to ensure the system is in a ready state (e.g., migrations, dependency installation).

## Run Report (mandatory — write this before closing)

Write `builder_reports/phase{N}_v{I}_{YYYYMMDD_HHMM}.md` in your current directory before closing. Always. Replace `{N}` and `{I}` with the Phase and Iteration numbers from your Session State.

```
# Builder Run Report
Phase: {N} | Iteration: {I} | {timestamp}

## What I did
- Which contracts I implemented/fixed
- Which source files I modified or created
- Which `abl-cmd` tools I ran and their results
- Summary of architectural changes made

## Friction log
- Every moment you had to probe, retry, or guess to understand system behavior or existing code
- Any spec ambiguities that forced assumptions
- Any context that was missing and required extra tool calls to discover
- Any command failures that required multiple attempts to resolve

## Turn self-assessment
- Rough split: how many turns were productive (writing code) vs discovery (figuring out what exists, response shapes, dependency mapping)
- What single piece of information, if provided upfront, would have saved the most turns this run?
```

## How to work
1. Read the "Current Phase Spec" in your context.
2. Read existing source files relevant to the spec before writing anything.
3. Implement what the contracts require — exactly, nothing more.
4. If `failed_specs.md` exists in your workspace (or you are informed of failures), fix those contracts first.
5. Use your project commands via `abl-cmd` to verify your work. If a health check is available, you must pass it. Capture and read any command output to identify and resolve issues.

## Rules
- Specs are law — implement exactly what they say.
- **You MUST ensure the implementation passes all provided health checks.**
- **You MUST write the Builder Run Report before finishing.**
- Do NOT run adversarial tests (that is the Verifier's job).
- Do NOT modify anything outside your working directory.
- When your code is stable, error-free, and passes health checks, stop.
--- ./docs/autonomous_build_loop.md ---
# Autonomous Build Loop (ABL)
## Concept Formalization v0.8

---

## Problem Statement

LLM-assisted development workflows that mirror human sprint team structures are inefficient because the orchestration burden falls on the human, role-playing generates fictional artifacts, and token usage is spent on organizational theater rather than intelligence. ABL eliminates all of this.

---

## Core Concept

A phase-based autonomous build loop where the human writes specs and performs final audit. Everything in between is automatic. The orchestration is handled by a Node.js CLI. The LLM engine is Gemini CLI.

---

## Mental Model

```
Specs → [abl phase N] → Verified output → Human audit → approve or refine specs → next phase
```

The human touches the system exactly twice per phase: spec input and audit output.

---

## Roles

**Builder** — reads the dynamically injected phase and iteration, pulls the current phase spec from `specs/`, reads the codebase. Implements contracts exactly. If `failed_specs.md` exists, fixes those first. Manages its own technical quality using project-defined `abl-cmd` tools (lint, typecheck, migrations). Produces a Builder Run Report before finishing.

**Verifier** — reads the dynamically injected phase and iteration, pulls all cumulative specs from `specs/`, tests the live running system. Prepares the test environment using project-defined `abl-cmd` tools (seeding, starting server). Reports failures via `failed_specs.md`. Never reads source code. Produces a Verifier Run Report before finishing.

There is no Rectifier. The Builder is the only entity that touches code.

---

## Information Boundaries

| Role | Reads | Never reads |
|---|---|---|
| Builder | specs/ + SRC_DIR codebase + failed_specs.md | TESTS_DIR, prompts/verifier.md |
| Verifier | specs/ + TESTS_DIR + failed_specs.md | SRC_DIR |

Boundaries are enforced by **Docker**. Each role runs in an isolated container. `specs/` is mounted read-only for both roles.

---

## Communication Channel: Builder ↔ Verifier

The Verifier passes failures to the Builder via `TESTS_DIR/failed_specs.md`. Each entry contains the failed contract, the exact input used, and the observed system behavior. Written entirely in spec language — no test code, no test output, no implementation details.

Format:

```
SPEC: POST /auth/login {valid credentials} → 200 {token}
INPUT: {"email": "test@test.com", "password": "correct"}
OBSERVED: 404

SPEC: GET /dashboard (valid token) → 200
INPUT: GET /dashboard headers: {Authorization: Bearer <token>}
OBSERVED: redirect /login
```

INPUT is mandatory — the Builder must know exactly what stimulus triggered the failure.

**Pass/fail signal:** The presence of `failed_specs.md` containing at least one `SPEC:` entry means failure. Absence or empty file means pass.

---

## Git Strategy

Two separate git repositories — one in SRC_DIR, one in TESTS_DIR. The Docker boundary extends into version control. Neither role can traverse the other's git history.

---

## File Structure

```
project/
├── [SRC_DIR]/                # configurable — default "src"
│   ├── .git/
│   ├── [all application code]
│   └── builder_reports/      # Builder's iteration reports
├── .abl/
│   ├── tests/                # Verifier's workspace (configurable)
│   │   ├── .git/
│   │   ├── failed_specs.md   # Written by Verifier on failure, deleted on pass
│   │   └── verifier_reports/ # Verifier's iteration reports
│   ├── specs/
│   │   ├── phase1.md
│   │   └── phaseN.md
│   ├── logs/                 # Runtime artifacts
│   │   ├── builder.log       # Full Gemini CLI JSON output from Builder (last run)
│   │   ├── verifier.log      # Full Gemini CLI JSON output from Verifier (last run)
│   │   └── tokens.csv        # Cumulative token usage across all phases
│   ├── project.md            # Human-written project description
│   └── project_map.txt       # Auto-generated before every LLM call
├── .env                      # Secrets — gitignored, never passed to LLMs
└── prompts/
    ├── builder.md
    └── verifier.md
```

---

## The Loop

```
abl phase PHASE=N:
    - Initialize SRC_DIR/.git and TESTS_DIR/.git if not present
    - Initialize .abl/logs/tokens.csv
    - Inject <<PHASE>> and <<ITERATION>> into prompts

outer loop (max 5 Verifier iterations):

    Builder runs in Docker (SRC_DIR + specs/)
    Builder implements specs
    Builder runs self-checks via abl-cmd
    Builder writes report to src/builder_reports/
    git commit SRC_DIR: "phaseN/build/step-X"

    Verifier runs in Docker (tests/ + specs/)
    Verifier prepares environment via abl-cmd
    Verifier writes and runs tests
    Verifier writes report to tests/verifier_reports/
    → failed_specs.md absent or empty: ✓ PASS — surface to human
    → failed_specs.md has SPEC entries: FAIL
      git commit TESTS_DIR: "phaseN/verify/step-X"
      next outer iteration

outer loop exhausted (5 Verifier fails): STUCK (contracts) → surface failed_specs.md
```

---

## Continuous Phases

`abl phase N` is idempotent and resumable. The loop always picks up from where it left off based on the internal `state.json`.

---

## Token Tracking

Every Gemini call appends a row to `.abl/logs/tokens.csv`. View cumulative totals:
```bash
abl costs
```

---

## Spec and Phase Requirements

### What a Spec Is

A spec is a behavioral contract. It defines a specific action and a specific expected result with no room for interpretation. The Verifier must be able to derive a test from it without any implementation knowledge.

### Spec Format

**Context** — natural language intent, architecture decisions, constraints.

**Contracts** — exact quasi-code action → result pairs. One action, one result, no ambiguity.

```
ACTION → EXPECTED RESULT
```

### Spec Rules

- First line must be `# Phase N: Title`
- Specs are append-only across phases
- Cumulative: the Verifier tests ALL phases every iteration, not just the current one

---

## Environment and Security

**Docker** is the enforcement layer. Builder sees only SRC_DIR and specs/. Verifier sees only TESTS_DIR and specs/.

**Secrets** live in `.env` at project root, gitignored. The runner reads `GEMINI_API_KEY` from `.env` and passes it as an environment variable to the container.

---

## Human Audit

The human audits the phase output as a complete experience. Problems are addressed by refining specs — never by patching code directly. All corrections become permanent spec knowledge.

--- ./package.json ---
{
  "name": "abl",
  "version": "2.0.0-alpha.1",
  "description": "Autonomous Build Loop — a phase-based AI development framework",
  "main": "src/index.js",
  "bin": {
    "abl": "./src/index.js"
  },
  "scripts": {
    "start": "node src/index.js"
  },
  "dependencies": {
    "chalk": "^4.1.2",
    "commander": "^12.0.0",
    "js-yaml": "^4.1.0"
  },
  "engines": {
    "node": ">=18.0.0"
  },
  "keywords": [
    "ai",
    "autonomous",
    "build",
    "loop",
    "gemini",
    "llm",
    "framework",
    "tdd"
  ],
  "license": "MIT"
}

--- ./README.md ---
# ABL — Autonomous Build Loop

A phase-based AI development framework. Write specs, run `abl phase N`, audit the output.

## How it works

```
Specs → [abl phase N] → Verified output → Human audit → next phase
```

The human touches the system exactly twice per phase: writing the spec and auditing the result. Everything in between — building, testing, retrying — is automatic.

ABL uses two isolated AI roles:

- **Builder** — reads specs, writes code, manages its own environment quality via `abl-cmd`. Runs in a Docker container with access only to your source directory.
- **Verifier** — reads specs, tests the live system adversarially, reports contract failures. Runs in a separate Docker container with access only to your tests directory.

Neither role can see the other's workspace.

## Requirements

- Node.js >= 18
- Docker (running)
- Gemini CLI (`npm install -g @google/generative-ai-cli`)
- A `GEMINI_API_KEY` in your project's `.env`

## Installation

```bash
npm install -g abl
```

## Quick Start

```bash
cd your-project
abl init
# Edit .abl/project.md and .abl/specs/phase1.md
abl phase 1
```

## Commands

```
abl init                    Initialize ABL in the current directory
abl phase <N>               Run phase N
abl phase <N> -b <model>    Override builder model
abl phase <N> -v <model>    Override verifier model
abl costs                   Show cumulative token usage
abl --help                  Show help
```

## Project Structure

After `abl init`:

```
your-project/
├── .abl/
│   ├── abl.config.yaml     # Project configuration — edit this
│   ├── project.md          # Project description — edit this
│   ├── specs/              # Your specs — write these
│   ├── tests/              # Verifier's workspace
│   ├── logs/
│   │   └── tokens.csv      # Cumulative token usage
│   ├── lean_settings.json  # Gemini CLI settings (auto-generated)
│   └── geminiignore.txt    # Gemini ignore rules (auto-generated)
├── src/                    # Your application source (Builder's workspace)
└── .env                    # GEMINI_API_KEY=your_key (gitignored)
```

## Configuration

`.abl/abl.config.yaml`:

```yaml
directories:
  src: ./src

models:
  builder: gemini-2.5-pro
  verifier: gemini-2.5-flash

builder_commands:
  health_check: npm run lint && npx tsc --noEmit
  start_dev: npm run dev

verifier_commands:
  seed: npm run db:seed

loop:
  max_verifier_iterations: 5
```

## Writing Specs

Specs are behavioral contracts. First line must be `# Phase N: Title`.

```markdown
# Phase 1: Authentication

Users must be able to register and log in. Sessions are JWT-based.
Passwords are hashed. Database is PostgreSQL.

### Contracts
POST /auth/register {email, password} → 201 {user_id}
POST /auth/register {existing email} → 409
POST /auth/login {valid credentials} → 200 {token}
POST /auth/login {invalid password} → 401
GET /dashboard (no token) → redirect /login
GET /dashboard (valid token) → 200
```

## How the Loop Works

```
abl phase N
  ↓
Builder:
  Builder writes code (Docker: src/ + specs/)
  Builder runs its own health checks (abl-cmd)
  Builder writes Run Report
  ↓
Verifier:
  Verifier resets environment (abl-cmd)
  Verifier tests all cumulative contracts adversarially
  Verifier writes failed_specs.md on failure
  Verifier writes Run Report
  ↓
Outer loop (max 5 Verifier iterations)
  → fail: commit, next outer iteration
  → pass: done ✓
  → exhausted: STUCK — see failed_specs.md
```

## Resume

`abl phase N` is idempotent. If a run was interrupted or got STUCK:

```bash
abl phase N   # automatically detects failed_specs.md and resumes
```

## Token Tracking

Every Gemini call is logged to `.abl/logs/tokens.csv`. View a summary:

```bash
abl costs
```

## Environment Variables

| Variable | Description |
|---|---|
| `GEMINI_API_KEY` | Required. Put in `.env` at project root. |
| `ABL_DEBUG` | Set to any value to print stack traces on errors. |

## v1 Reference

The original Makefile-based v1 is archived in `v1/` for reference. It requires Linux + firejail and is not actively maintained.

## License

MIT

--- ./v1/Makefile ---
PHASE          ?= 1
BUILDER_MODEL  ?= gemini-3-flash-preview
VERIFIER_MODEL ?= gemini-3-flash-preview
SPECS          := $(wildcard specs/phase*.md)
GEMINI         := $(HOME)/.nvm/versions/node/v22.21.0/bin/gemini
FIREJAIL       := firejail --noprofile \
                  --whitelist=$(HOME)/.gemini \
                  --whitelist=$(HOME)/.nvm \
                  --whitelist=$(shell pwd)/.abl
SRC_DIR        := $(shell bash abl.config.sh src_dir)
TESTS_DIR      := $(shell bash abl.config.sh tests_dir)
GEMINI_API_KEY := $(shell grep ^GEMINI_API_KEY .env 2>/dev/null | cut -d= -f2)
LEAN_CONFIG    := $(shell pwd)/.abl/lean_settings.json

# ── Helpers ────────────────────────────────────────────────────────────────

_setup:
	@mkdir -p .abl logs specs
	@sed -i "s|ABSOLUTE_PATH_PLACEHOLDER|$(shell pwd)|g" .abl/lean_settings.json 2>/dev/null || true

_extract_tokens:
	@log=logs/$(ROLE).log; \
	ts=$$(date '+%Y-%m-%d %H:%M:%S'); \
	json=$$(echo "{"; awk '/"stats":/{f=1} f; /^}/{if(f) exit}' "$$log"); \
	session=$$(grep -o '"session_id": *"[^"]*"' "$$log" | head -1 | grep -o '"[^"]*"$$' | tr -d '"'); \
	if echo "$$json" | jq -e '.stats' > /dev/null 2>&1; then \
		echo "$$json" | jq -r --arg ts "$$ts" --arg phase "$(PHASE)" --arg step "$(STEP)" \
		--arg role "$(ROLE)" --arg session "$$session" \
		'.stats.models | to_entries[] | [$$ts, $$phase, $$step, $$role, .key, (.value.tokens.input // 0), (.value.tokens.candidates // 0), (.value.tokens.cached // 0), (.value.tokens.total // 0), $$session] | @csv' \
		>> logs/tokens.csv; \
	else \
		echo "\"$$ts\",\"$(PHASE)\",\"$(STEP)\",\"$(ROLE)\",\"unknown\",\"ERROR\",0,0,0,0,\"none\"" >> logs/tokens.csv; \
	fi

_last_session:
	@tail -1 logs/tokens.csv | cut -d',' -f10 | tr -d '"'

_map:
	@tree $(SRC_DIR)/ -I 'node_modules|.git' --dirsfirst > project_map.txt
	@echo "---" >> project_map.txt
	@bash abl.config.sh map_deps >> project_map.txt

_index:
	@head -1 specs/phase$(PHASE).md >> specs/index.md

# ── Roles ──────────────────────────────────────────────────────────────────

_build:
	@$(MAKE) --no-print-directory _map
	@echo "  ⚙  Builder running..."
	@cat prompts/builder.md project.md project_map.txt specs/index.md \
	| $(FIREJAIL) --whitelist=$(shell pwd)/$(SRC_DIR) --whitelist=$(shell pwd)/specs \
	  env GEMINI_API_KEY=$(GEMINI_API_KEY) \
	  GEMINI_CLI_SYSTEM_SETTINGS_PATH=$(LEAN_CONFIG) \
	  $(GEMINI) -m $(BUILDER_MODEL) \
	  --include-directories $(shell pwd)/$(SRC_DIR),$(shell pwd)/specs \
	  -y --output-format json \
	  -p "Execute your build instructions for phase $(PHASE)." \
	  > logs/builder.log 2>&1
	@$(MAKE) --no-print-directory _extract_tokens ROLE=builder STEP=$(STEP)
	@cd $(SRC_DIR) && git add -A && { git diff --cached --quiet || git commit -m "phase$(PHASE)/build/step-$(STEP)/pre-deterministic" --quiet; } && cd ..
	@echo "  ✓  Builder done"


_build_with_context:
	@$(MAKE) --no-print-directory _map
	@echo "  ⚙  Builder running with failure context..."
	@cat prompts/builder.md project.md project_map.txt specs/index.md \
	  $(shell [ -f $(TESTS_DIR)/failed_specs.md ] && echo $(TESTS_DIR)/failed_specs.md) \
	  $(shell [ -f logs/health.log ] && echo logs/health.log) \
	| $(FIREJAIL) --whitelist=$(shell pwd)/$(SRC_DIR) --whitelist=$(shell pwd)/specs \
	  env GEMINI_API_KEY=$(GEMINI_API_KEY) \
	  GEMINI_CLI_SYSTEM_SETTINGS_PATH=$(LEAN_CONFIG) \
	  $(GEMINI) -m $(BUILDER_MODEL) \
	  --include-directories $(shell pwd)/$(SRC_DIR),$(shell pwd)/specs \
	  -y --output-format json \
	  -p "Execute your build instructions for phase $(PHASE)." \
	  > logs/builder.log 2>&1
	@$(MAKE) --no-print-directory _extract_tokens ROLE=builder STEP=$(STEP)
	@cd $(SRC_DIR) && git add -A && { git diff --cached --quiet || git commit -m "phase$(PHASE)/build/step-$(STEP)/pre-deterministic" --quiet; } && cd ..
	@echo "  ✓  Builder done"

_verify:
	@$(MAKE) --no-print-directory _map
	@echo "  ⚙  Verifier running..."
	@cat prompts/verifier.md project.md project_map.txt specs/index.md \
	  $(shell [ -f $(TESTS_DIR)/failed_specs.md ] && echo $(TESTS_DIR)/failed_specs.md) \
	| $(FIREJAIL) --whitelist=$(shell pwd)/$(TESTS_DIR) --whitelist=$(shell pwd)/specs \
	  env GEMINI_API_KEY=$(GEMINI_API_KEY) \
	  GEMINI_CLI_SYSTEM_SETTINGS_PATH=$(LEAN_CONFIG) \
	  $(GEMINI) -m $(VERIFIER_MODEL) \
	  --include-directories $(shell pwd)/$(TESTS_DIR),$(shell pwd)/specs \
	  -y --output-format json \
	  -p "Run your test suite for phase $(PHASE). Write failed_specs.md on failure, delete it on pass." \
	  > logs/verifier.log 2>&1
	@$(MAKE) --no-print-directory _extract_tokens ROLE=verifier STEP=$(STEP)
	@cd $(TESTS_DIR) && git add -A && { git diff --cached --quiet || git commit -m "phase$(PHASE)/verify/step-$(STEP)/results" --quiet; } && cd ..
	@echo "  ✓  Verifier done"

# ── Main loop ──────────────────────────────────────────────────────────────

costs:
	@if [ ! -f logs/tokens.csv ]; then echo "No token data yet — run a phase first."; exit 0; fi
	@echo ""
	@echo "━━━ Token Usage Summary ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
	@awk -F',' 'NR>1 && $$6!="0" { \
		gsub(/"/, "", $$4); gsub(/"/, "", $$5); \
		input+=$$6; candidates+=$$7; cached+=$$8; total+=$$9; calls++ \
	} END { \
		printf "  Total calls:      %d\n", calls; \
		printf "  Input tokens:     %d\n", input; \
		printf "  Output tokens:    %d\n", candidates; \
		printf "  Cached tokens:    %d\n", cached; \
		printf "  Total tokens:     %d\n", total; \
	}' logs/tokens.csv
	@echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
	@echo "  (Apply current model pricing to token counts above)"
	@echo ""

phase:
	@[ -d $(SRC_DIR)/.git ]   || git -C $(SRC_DIR) init --quiet
	@[ -d $(TESTS_DIR)/.git ] || git -C $(TESTS_DIR) init --quiet
	@$(MAKE) --no-print-directory _setup
	@[ -f logs/tokens.csv ] || echo "timestamp,phase,step,role,model,input,candidates,cached,total,session_id" > logs/tokens.csv
	@touch specs/index.md
	@$(MAKE) --no-print-directory _index
	@if [ -f logs/health.log ] || [ -f $(TESTS_DIR)/failed_specs.md ]; then \
		echo ""; \
		echo "↺  Resuming phase $(PHASE) from prior state..."; \
		[ -f logs/health.log ]              && echo "   → health.log found — Builder will receive health errors"; \
		[ -f $(TESTS_DIR)/failed_specs.md ] && echo "   → failed_specs.md found — Builder will receive contract failures"; \
	fi
	@for vi in 1 2 3 4 5; do \
		echo ""; \
		echo "━━━ Verifier iteration $$vi / 5 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"; \
		health_passed=0; \
		echo "  ⚙  Resetting DB state..."; \
		bash abl.config.sh reset_state || { echo "  ✗  DB reset failed — aborting"; exit 1; }; \
		echo "  ✓  DB ready"; \
		for hi in 1 2 3 4 5 6 7 8 9 10; do \
			echo "  ── Build / health attempt $$hi / 10"; \
			if [ $$vi -eq 1 ] && [ $$hi -eq 1 ] && [ ! -f logs/health.log ] && [ ! -f $(TESTS_DIR)/failed_specs.md ]; then \
				$(MAKE) --no-print-directory _build STEP=$$vi || exit 1; \
			else \
				$(MAKE) --no-print-directory _build_with_context STEP=$$vi || exit 1; \
			fi; \
			echo "  ⚙  Health check..."; \
			if bash abl.config.sh health_check; then \
				echo "  ✓  Health check passed"; \
				rm -f logs/health.log; \
				cd $(SRC_DIR) && git add -A && \
					{ git diff --cached --quiet || git commit -m "phase$(PHASE)/build/step-$$vi/post-deterministic" --quiet; } && cd ..; \
				health_passed=1; \
				break; \
			else \
				echo "  ✗  Health check failed (attempt $$hi) — retrying..."; \
			fi; \
		done; \
		if [ $$health_passed -eq 0 ]; then \
			echo ""; \
			echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"; \
			echo "✗  STUCK — Builder could not pass health check after 10 attempts"; \
			echo "    → see logs/health.log"; \
			echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"; \
			exit 1; \
		fi; \
		echo "  ⚙  Resetting DB state for Verifier..."; \
		bash abl.config.sh reset_state || { echo "  ✗  DB reset failed — aborting"; exit 1; }; \
		echo "  ✓  DB ready"; \
		echo "  ⚙  Starting dev server..."; \
		bash abl.config.sh start_dev; \
		echo "  ✓  Dev server ready"; \
		$(MAKE) --no-print-directory _verify STEP=$$vi; \
		bash abl.config.sh stop_dev; \
		echo "  ✓  Dev server stopped"; \
		if [ ! -f $(TESTS_DIR)/failed_specs.md ] || ! grep -q "SPEC:" $(TESTS_DIR)/failed_specs.md; then \
			echo ""; \
			echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"; \
			echo "✓  Phase $(PHASE) passed"; \
			echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"; \
			exit 0; \
		fi; \
		echo "  ✗  Contracts failed — see $(TESTS_DIR)/failed_specs.md"; \
	done; \
	bash abl.config.sh stop_dev 2>/dev/null; \
	echo ""; \
	echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"; \
	echo "✗  STUCK — Builder could not satisfy contracts after 5 Verifier iterations"; \
	echo "    → see $(TESTS_DIR)/failed_specs.md"; \
	echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"; \
	exit 1
--- ./v1/abl.config.sh ---
#!/bin/bash
# abl.config.sh — project-specific configuration for ABL
# Edit this file per project. Never edit the Makefile.

COMMAND=${1}

case "$COMMAND" in

  # ── Path configuration ────────────────────────────────────────────────────
  # Return the source code directory (relative to project root)
  src_dir)
    echo "src"
    ;;

  # Return the tests directory (relative to project root)
  tests_dir)
    echo "tests"
    ;;

  # ── Dev server ────────────────────────────────────────────────────────────
  start_dev)
    mkdir -p logs
    kill $(lsof -ti:3000) 2>/dev/null
    rm -f src/.next/dev/lock
    sleep 1
    cd src
    npm run dev > ../logs/dev.log 2>&1 &
    echo $! > ../logs/dev.pid
    cd ..
    sleep 5
    ;;

  stop_dev)
    kill -- -$(cat logs/dev.pid) 2>/dev/null
    pkill -f "next dev" 2>/dev/null
    kill $(lsof -ti:3000) 2>/dev/null
    rm -f src/.next/dev/lock
    ;;

  # ── Deterministic health checks ───────────────────────────────────────────
  # All output captured to logs/health.log
  # Return non-zero if any check fails
  health_check)
    mkdir -p logs
    > logs/health.log

    cd src

    echo "=== LINT ===" >> ../logs/health.log
    npm run lint >> ../logs/health.log 2>&1
    lint_exit=$?

    echo "=== TYPECHECK ===" >> ../logs/health.log
    npx tsc --noEmit >> ../logs/health.log 2>&1
    tsc_exit=$?

    # Add more checks as needed:
    # echo "=== UNIT TESTS ===" >> ../logs/health.log
    # npm test -- --passWithNoTests >> ../logs/health.log 2>&1
    # test_exit=$?

    cd ..
    [ $lint_exit -eq 0 ] && [ $tsc_exit -eq 0 ]
    ;;

  # ── State reset ───────────────────────────────────────────────────────────
  # Runs before every Verifier iteration
  # No-op if stateless, otherwise reset DB/cache/queues
  reset_state)
    cd src
    npm run db:seed || { echo "  ✗  Seed failed — aborting"; cd ..; exit 1; }
    cd ..
    ;;

  # ── Dependency map ────────────────────────────────────────────────────────
  # Appended to project_map.txt before every LLM call
  map_deps)
    cat src/package.json
    ;;

  *)
    echo "Unknown command: $COMMAND"
    exit 1
    ;;

esac
--- ./src/index.js ---
#!/usr/bin/env node

'use strict';

const { program } = require('commander');
const { version } = require('../package.json');

const runCommand     = require('./commands/run');
const phaseCommand   = require('./commands/phase');
const initCommand    = require('./commands/init');
const costsCommand   = require('./commands/costs');

program
  .name('abl')
  .description('Autonomous Build Loop — a phase-based AI development framework')
  .version(version);

program
  .command('init')
  .description('Initialize a new ABL project in the current directory')
  .action(initCommand);

program
  .command('run')
  .description('Run the next pending phase, or resume an interrupted one')
  .option('-b, --builder-model <model>',  'Override builder model')
  .option('-v, --verifier-model <model>', 'Override verifier model')
  .option('--cwd <path>', 'Run as if from this directory')
  .action(runCommand);

program
  .command('phase <n>')
  .description('Run a specific phase by number')
  .option('-b, --builder-model <model>',  'Override builder model')
  .option('-v, --verifier-model <model>', 'Override verifier model')
  .option('--cwd <path>', 'Run as if from this directory')
  .action(phaseCommand);

program
  .command('costs')
  .description('Show cumulative token usage summary')
  .option('--cwd <path>', 'Run as if from this directory')
  .action(costsCommand);

program.parse(process.argv);
--- ./src/commands/run.js ---
'use strict';

const chalk        = require('chalk');
const { loadConfig }        = require('../lib/config');
const { runPhase }          = require('../lib/loop');
const state                 = require('../lib/state');

async function runCommand(opts) {
  if (process.env.ABL_DEBUG) console.log(chalk.dim('DEBUG: runCommand started.'));
  let config;
  try {
    config = loadConfig(opts.cwd);
    if (process.env.ABL_DEBUG) console.log(chalk.dim(`DEBUG: Config loaded from ${config.resolved.root}`));
  } catch (e) {
    console.error(chalk.red(`Error: ${e.message}`));
    process.exit(1);
  }

  // Sync state with git before deciding what to do
  let s = state.read(config);
  if (process.env.ABL_DEBUG) console.log(chalk.dim('DEBUG: Current state read.'));
  s = state.syncWithGit(config, s);
  if (process.env.ABL_DEBUG) console.log(chalk.dim('DEBUG: State synced with git.'));
  state.write(config, s);

  // Resolve what to do next
  const next = state.resolveNextAction(config, s);
  if (process.env.ABL_DEBUG) console.log(chalk.dim(`DEBUG: Next action resolved: ${JSON.stringify(next)}`));

  if (next.action === 'done') {
    if (next.reason === 'no_specs') {
      console.log('');
      console.log(chalk.yellow('! No spec files found.'));
      console.log(chalk.dim(`  Add your first spec to ${config.resolved.specsDir}/phase1.md`));
      console.log('');
    } else {
      console.log('');
      console.log(chalk.green('━'.repeat(57)));
      console.log(chalk.green('✓  All phases complete. Nothing to do.'));
      console.log(chalk.dim(`   Completed: phases ${s.phases_completed.join(', ')}`));
      console.log(chalk.green('━'.repeat(57)));
      console.log('');
    }
    return;
  }

  const { phase } = next;

  if (next.action === 'resume') {
    console.log('');
    console.log(chalk.yellow(`↺  Resuming phase ${phase} (${next.status})...`));
  } else {
    console.log('');
    console.log(chalk.bold(`▶  Running phase ${phase}...`));
  }

  // Apply model overrides
  if (opts.builderModel)  config.models.builder  = opts.builderModel;
  if (opts.verifierModel) config.models.verifier = opts.verifierModel;

  try {
    await runPhase(config, phase, {
      builderModel:  config.models.builder,
      verifierModel: config.models.verifier,
    });
  } catch (e) {
    console.error(chalk.red(`\nFatal: ${e.message}`));
    if (process.env.ABL_DEBUG) console.error(e.stack);
    process.exit(1);
  }

  // Phase completed — check if more phases to run
  const updated  = state.syncWithGit(config, state.read(config));
  const nextNext = state.resolveNextAction(config, updated);

  if (nextNext.action === 'run') {
    console.log('');
    console.log(chalk.dim(`  Phase ${phase} done. Run ${chalk.bold('abl run')} again to continue with phase ${nextNext.phase}.`));
    console.log('');
  } else if (nextNext.action === 'done' && nextNext.reason === 'all_complete') {
    console.log('');
    console.log(chalk.green('━'.repeat(57)));
    console.log(chalk.green('✓  All phases complete.'));
    console.log(chalk.green('━'.repeat(57)));
    console.log('');
  }
}

module.exports = runCommand;
--- ./src/commands/costs.js ---
'use strict';

const { loadConfig } = require('../lib/config');
const { printCosts } = require('../lib/tokens');
const chalk          = require('chalk');

function costsCommand(opts) {
  let config;
  try {
    config = loadConfig(opts.cwd);
  } catch (e) {
    console.error(chalk.red(`Error: ${e.message}`));
    process.exit(1);
  }

  printCosts(config);
}

module.exports = costsCommand;
--- ./src/commands/phase.js ---
'use strict';

const chalk          = require('chalk');
const { loadConfig } = require('../lib/config');
const { runPhase }   = require('../lib/loop');
const state          = require('../lib/state');

async function phaseCommand(n, opts) {
  const phase = parseInt(n);
  if (isNaN(phase) || phase < 1) {
    console.error(chalk.red('Error: phase must be a positive integer'));
    process.exit(1);
  }

  let config;
  try {
    config = loadConfig(opts.cwd);
  } catch (e) {
    console.error(chalk.red(`Error: ${e.message}`));
    process.exit(1);
  }

  // Sync state with git
  let s = state.read(config);
  s = state.syncWithGit(config, s);
  state.write(config, s);

  // Warn if a different phase is in progress
  if (s.current && s.current.phase !== phase &&
      s.current.status !== state.STATUS.COMPLETED &&
      s.current.status !== state.STATUS.STUCK) {
    console.log('');
    console.log(chalk.yellow(`! Phase ${s.current.phase} is currently in progress (${s.current.status}).`));
    console.log(chalk.dim(`  Forcing phase ${phase}. State for phase ${s.current.phase} will be preserved.`));
  }

  // Warn if phase already completed
  if (s.phases_completed.includes(phase)) {
    console.log('');
    console.log(chalk.yellow(`! Phase ${phase} is already marked complete. Re-running.`));
    state.resetPhase(config, phase);
  }

  if (opts.builderModel)  config.models.builder  = opts.builderModel;
  if (opts.verifierModel) config.models.verifier = opts.verifierModel;

  try {
    await runPhase(config, phase, {
      builderModel:  config.models.builder,
      verifierModel: config.models.verifier,
    });
  } catch (e) {
    console.error(chalk.red(`\nFatal: ${e.message}`));
    if (process.env.ABL_DEBUG) console.error(e.stack);
    process.exit(1);
  }
}

module.exports = phaseCommand;
--- ./src/commands/init.js ---
'use strict';

const fs = require('fs');
const path = require('path');
const readline = require('readline');
const chalk = require('chalk');

const PACKAGE_ROOT = path.join(__dirname, '..', '..');
const TEMPLATE_DIR = path.join(PACKAGE_ROOT, '.abl');

async function initCommand() {
  const rl = readline.createInterface({ input: process.stdin, output: process.stdout });
  const ask = (q) => new Promise(res => rl.question(q, res));
  console.log(chalk.bold('\nABL Initialize'));
  const src = await ask('Source directory name? (default: src): ') || 'src';
  const tests = await ask('Tests directory name? (default: tests): ') || 'tests';
  rl.close();

  const cwd = process.cwd();
  const ablDir = path.join(cwd, '.abl');
  [ablDir, path.join(ablDir, 'specs'), path.join(ablDir, 'logs'), path.join(cwd, src), path.join(cwd, tests)].forEach(d => fs.mkdirSync(d, { recursive: true }));

  const copy = (file, dest = file) => {
    const s = path.join(TEMPLATE_DIR, file);
    if (fs.existsSync(s)) {
      let c = fs.readFileSync(s, 'utf8');
      if (file === 'abl.config.yaml') c = c.replace(/src: .*/, `src: ./${src}`).replace(/tests: .*/, `tests: ./${tests}`);
      fs.writeFileSync(path.join(ablDir, dest), c);
    }
  };
  ['abl.config.yaml', 'lean_settings.json', 'geminiignore.txt'].forEach(f => copy(f));

  fs.writeFileSync(path.join(ablDir, 'project.md'), '# Project Name\n');
  fs.writeFileSync(path.join(ablDir, 'state.json'), JSON.stringify({ phases_completed: [], current: null, phase_titles: {} }, null, 2));
  fs.writeFileSync(path.join(ablDir, 'specs', 'phase1.md'), '# Phase 1: Setup\n\n### Contracts\n- Action -> Result\n');

  console.log(chalk.green('\n✓ ABL Initialized.'));
  console.log('\n  Available Commands:');
  console.log(chalk.dim('  abl run         Run or resume pending phase'));
  console.log(chalk.dim('  abl phase <N>   Run specific phase'));
  console.log(chalk.dim('  abl costs       Show token usage\n'));
}

module.exports = initCommand;
--- ./src/lib/tokens.js ---
'use strict';

const fs = require('fs');
const path = require('path');
const chalk = require('chalk');

const CSV_HEADER = 'timestamp,phase,step,role,model,input,candidates,cached,total,session_id\n';

function extractTokens(config, logPath, role, phase, step) {
  const { tokensCsv } = config.resolved;
  if (!fs.existsSync(tokensCsv)) fs.writeFileSync(tokensCsv, CSV_HEADER);
  
  let raw = '';
  try { raw = fs.readFileSync(logPath, 'utf8'); } catch (e) { return; }

  const sessionMatch = raw.match(/"session_id"\s*:\s*"([^"]+)"/);
  const sessionId = sessionMatch ? sessionMatch[1] : 'none';

  const statsMatch = raw.match(/\{[\s\S]*?"stats"[\s\S]*?\}/);
  if (!statsMatch) return;

  try {
    const parsed = JSON.parse(statsMatch[0]);
    const ts = new Date().toISOString().replace('T', ' ').slice(0, 19);
    for (const [model, data] of Object.entries(parsed.stats.models)) {
      const t = data.tokens || {};
      const row = [`"${ts}"`,`"${phase}"`,`"${step}"`,`"${role}"`,`"${model}"`,t.input||0,t.candidates||0,t.cached||0,t.total||0,`"${sessionId}"`].join(',');
      fs.appendFileSync(tokensCsv, row + '\n');
    }
  } catch (e) {}
}

function printCosts(config) {
  if (!fs.existsSync(config.resolved.tokensCsv)) return console.log('No costs logged.');
  const lines = fs.readFileSync(config.resolved.tokensCsv, 'utf8').trim().split('\n').slice(1);
  let totals = { calls: 0, input: 0, output: 0, total: 0 };
  lines.forEach(l => {
    const c = l.split(',');
    if (c[5] === 'ERROR') return;
    totals.calls++;
    totals.input += parseInt(c[5]) || 0;
    totals.output += parseInt(c[6]) || 0;
    totals.total += parseInt(c[8]) || 0;
  });
  console.log(chalk.bold(`\n  Total Calls: ${totals.calls}\n  Total Tokens: ${totals.total.toLocaleString()}\n`));
}

module.exports = { extractTokens, printCosts };
--- ./src/lib/logger.js ---
'use strict';

const chalk = require('chalk');

const DIVIDER     = '━'.repeat(57);
const DIVIDER_SML = '─'.repeat(57);

module.exports = {
  phase(n, total) {
    console.log('');
    console.log(chalk.bold(`━━━ Verifier iteration ${n} / ${total} ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━`));
  },

  attempt(hi, maxH) {
    console.log(`  ${chalk.dim('──')} Build / health attempt ${hi} / ${maxH}`);
  },

  info(msg) {
    console.log(`  ${chalk.cyan('⚙')}  ${msg}`);
  },

  success(msg) {
    console.log(`  ${chalk.green('✓')}  ${msg}`);
  },

  fail(msg) {
    console.log(`  ${chalk.red('✗')}  ${msg}`);
  },

  warn(msg) {
    console.log(`  ${chalk.yellow('!')}  ${msg}`);
  },

  resume(items) {
    console.log('');
    console.log(chalk.yellow('↺  Resuming from prior state...'));
    for (const item of items) {
      console.log(`   ${chalk.dim('→')} ${item}`);
    }
  },

  stuck(reason, hint) {
    console.log('');
    console.log(chalk.red(DIVIDER));
    console.log(chalk.red(`✗  STUCK — ${reason}`));
    console.log(chalk.dim(`    → ${hint}`));
    console.log(chalk.red(DIVIDER));
  },

  passed(phase) {
    console.log('');
    console.log(chalk.green(DIVIDER));
    console.log(chalk.green(`✓  Phase ${phase} passed`));
    console.log(chalk.green(DIVIDER));
  },

  contractsFailed(testsDir) {
    console.log(`  ${chalk.red('✗')}  Contracts failed — see ${testsDir}/failed_specs.md`);
  },

  divider() {
    console.log(chalk.dim(DIVIDER_SML));
  },

  blank() {
    console.log('');
  }
};
--- ./src/lib/map.js ---
'use strict';

const { execSync } = require('child_process');
const fs = require('fs');

function generateMap(config) {
  const { srcDir, projectMap } = config.resolved;
  
  try {
    // Generate file tree
    let map = execSync(`tree "${srcDir}" -I 'node_modules|.git' --dirsfirst`, { encoding: 'utf8' });
    
    // Append dependency info if available
    const pkgPath = require('path').join(srcDir, 'package.json');
    if (fs.existsSync(pkgPath)) {
      map += '\n--- Dependencies ---\n';
      map += fs.readFileSync(pkgPath, 'utf8');
    }

    fs.writeFileSync(projectMap, map);
  } catch (e) {
    fs.writeFileSync(projectMap, `Error generating map: ${e.message}`);
  }
}

module.exports = { generateMap };
--- ./src/lib/context.js ---
'use strict';

const fs = require('fs');
const path = require('path');
const stateLib = require('./state');
const { buildCommandContext } = require('./config');

function buildPipedContext(config, role, phase, iteration) {
  const state = stateLib.read(config);
  
  const projectMd = fs.existsSync(path.join(config.resolved.ablDir, 'project.md'))
    ? fs.readFileSync(path.join(config.resolved.ablDir, 'project.md'), 'utf8')
    : '# Project\nNo description provided.';

  const projectMap = fs.existsSync(config.resolved.projectMap)
    ? fs.readFileSync(config.resolved.projectMap, 'utf8')
    : 'No project map available.';

  const rolePromptPath = path.join(config.resolved.promptsDir, `${role}.md`);
  const rolePrompt = fs.existsSync(rolePromptPath)
    ? fs.readFileSync(rolePromptPath, 'utf8')
    : `You are the ${role}. Implement the requested phase.`;

  const currentSpecPath = path.join(config.resolved.specsDir, `phase${phase}.md`);
  const currentSpec = fs.existsSync(currentSpecPath)
    ? fs.readFileSync(currentSpecPath, 'utf8')
    : 'No spec found for this phase.';

  const historyLines = Object.entries(state.phase_titles)
    .sort((a, b) => parseInt(a[0]) - parseInt(b[0]))
    .map(([num, title]) => {
      const status = state.phases_completed.includes(parseInt(num)) ? '[COMPLETED]' : '[PENDING]';
      return `- Phase ${num}: ${title} ${status}`;
    });

  const cmdContext = buildCommandContext(config, role);

  return [
    rolePrompt,
    '',
    '# Project Context',
    projectMd,
    '',
    '# System Topology',
    projectMap,
    '',
    '# Session State',
    `Current Phase: ${phase} (${state.phase_titles[phase] || 'Unknown'})`,
    `Current Iteration: ${iteration}`,
    '',
    '# Phase History',
    historyLines.join('\n'),
    '',
    '# Current Phase Spec (CONTRACTS)',
    currentSpec,
    '',
    '# Available Commands',
    cmdContext
  ].join('\n');
}

module.exports = { buildPipedContext };
--- ./src/lib/executor.js ---
'use strict';

const { spawnSync, execSync } = require('child_process');
const fs = require('fs');
const path = require('path');
const { buildPipedContext } = require('./context');
const { generateMap } = require('./map');

/**
 * Creates a temporary abl-cmd executable script inside the workspace
 * to facilitate local execution without a proxy server.
 */
function setupAblCmd(config, role, workspace) {
  const commands = role === 'builder' ? config.builder_commands : config.verifier_commands;
  const specsDir = config.resolved.specsDir;

  let scriptContent = `#!/usr/bin/env node
const { execSync } = require('child_process');
const fs = require('fs');
const path = require('path');

const cmd = process.argv[2];
const args = process.argv.slice(3);

if (cmd === 'get-spec') {
  const phase = args[0];
  const p = path.join('${specsDir.replace(/\\/g, '\\\\')}', \`phase\${phase}.md\`);
  if (fs.existsSync(p)) {
    console.log(fs.readFileSync(p, 'utf8'));
    process.exit(0);
  } else {
    console.error('Spec not found');
    process.exit(1);
  }
}

const commands = ${JSON.stringify(commands || {})};
if (commands[cmd]) {
  try {
    const out = execSync(commands[cmd].command, { stdio: 'inherit', encoding: 'utf8' });
    process.exit(0);
  } catch (e) {
    process.exit(1);
  }
}

console.error('Unknown command: ' + cmd);
process.exit(1);
`;

  const binDir = path.join(workspace, '.abl_bin');
  if (!fs.existsSync(binDir)) fs.mkdirSync(binDir, { recursive: true });
  
  const cmdPath = path.join(binDir, 'abl-cmd');
  fs.writeFileSync(cmdPath, scriptContent, { mode: 0o755 });
  return binDir;
}

async function runRole(role, config, phase, opts) {
  const workspace = role === 'builder' ? config.resolved.srcDir : config.resolved.testsDir;
  const iteration = opts.iteration || 1;
  const logPath = path.join(config.resolved.logsDir, `${role}.log`);

  // 1. Refresh Metadata
  generateMap(config);

  // 2. Prepare Context
  const fullPrompt = buildPipedContext(config, role, phase, iteration);
  
  // 3. Setup local abl-cmd
  const binDir = setupAblCmd(config, role, workspace);

  // 4. Execute Gemini CLI
  const env = {
    ...process.env,
    PATH: `${binDir}${path.delimiter}${process.env.PATH}`,
    GEMINI_CLI_SYSTEM_SETTINGS_PATH: path.join(config.resolved.ablDir, 'lean_settings.json'),
    GEMINI_API_KEY: process.env.GEMINI_API_KEY
  };

  const model = opts.model || (role === 'builder' ? config.models.builder : config.models.verifier);
  const args = [
    '-m', model,
    '-y', // Auto-confirm tool usage
    '--output-format', 'json',
    '-p', fullPrompt,
    'Execute Task'
  ];

  const result = spawnSync('gemini', args, {
    cwd: workspace,
    env,
    encoding: 'utf8',
    maxBuffer: 10 * 1024 * 1024 // 10MB
  });

  fs.writeFileSync(logPath, result.stdout + result.stderr);

  if (result.status !== 0) {
    throw new Error(`${role} failed with exit code ${result.status}. See logs.`);
  }

  return logPath;
}

module.exports = {
  runBuilder: (config, phase, opts) => runRole('builder', config, phase, opts),
  runVerifier: (config, phase, opts) => runRole('verifier', config, phase, opts)
};
--- ./src/lib/config.js ---
'use strict';

const fs = require('fs');
const path = require('path');
const yaml = require('js-yaml');

const DEFAULTS = {
  models: { builder: 'gemini-2.0-flash', verifier: 'gemini-2.0-flash' },
  loop: { max_verifier_iterations: 5 }
};

function loadConfig(cwd = process.cwd()) {
  const ablDir = path.join(cwd, '.abl');
  const configFile = path.join(ablDir, 'abl.config.yaml');
  if (!fs.existsSync(configFile)) throw new Error('No ABL project found. Run "abl init".');
  
  const raw = yaml.load(fs.readFileSync(configFile, 'utf8')) || {};
  const dirs = raw.directories || {};

  return {
    ...DEFAULTS,
    ...raw,
    models: raw.models ? { ...DEFAULTS.models, ...raw.models } : DEFAULTS.models,
    loop: { ...DEFAULTS.loop, ...(raw.loop || {}) },
    resolved: {
      root: cwd,
      ablDir,
      specsDir: path.join(ablDir, 'specs'),
      logsDir: path.join(ablDir, 'logs'),
      promptsDir: path.join(cwd, 'prompts'),
      tokensCsv: path.join(ablDir, 'logs', 'tokens.csv'),
      projectMap: path.join(ablDir, 'project_map.txt'),
      stateFile: path.join(ablDir, 'state.json'),
      srcDir: path.resolve(cwd, dirs.src || './src'),
      testsDir: path.resolve(cwd, dirs.tests || './tests')
    }
  };
}

function buildCommandContext(config, role) {
  const commands = role === 'builder' ? config.builder_commands : config.verifier_commands;
  const lines = [
    'You have access to "abl-cmd" via the system shell.',
    'Usage: abl-cmd <name> [args]',
    '- abl-cmd get-spec <N> : Returns the content of phaseN.md spec'
  ];
  if (commands) {
    Object.entries(commands).forEach(([name, def]) => {
      lines.push(`- abl-cmd ${name} : ${def.description || name}`);
    });
  }
  return lines.join('\n');
}

module.exports = { loadConfig, buildCommandContext };
--- ./src/lib/state.js ---
'use strict';

const fs = require('fs');
const path = require('path');
const { execSync } = require('child_process');

const STATUS = { 
  CLEAN: 'clean', 
  BUILDING: 'building', 
  VERIFYING: 'verifying', 
  CONTRACTS_FAILED: 'contracts_failed', 
  STUCK: 'stuck', 
  COMPLETED: 'completed' 
};

function read(config) {
  const p = config.resolved.stateFile;
  let state = { phases_completed: [], current: null, last_updated: null, phase_titles: {} };
  
  if (fs.existsSync(p)) {
    try { state = JSON.parse(fs.readFileSync(p, 'utf8')); } catch (e) {}
  }

  const specsDir = config.resolved.specsDir;
  if (fs.existsSync(specsDir)) {
    fs.readdirSync(specsDir).filter(f => f.match(/^phase(\d+)\.md$/)).forEach(f => {
      const n = f.match(/\d+/)[0];
      const content = fs.readFileSync(path.join(specsDir, f), 'utf8');
      state.phase_titles[n] = content.split('\n')[0].replace(/^#\s*/, '').trim();
    });
  }
  return state;
}

function write(config, state) {
  state.last_updated = new Date().toISOString();
  fs.writeFileSync(config.resolved.stateFile, JSON.stringify(state, null, 2));
}

function syncWithGit(config, state) {
  if (!fs.existsSync(path.join(config.resolved.srcDir, '.git'))) return state;
  try {
    const log = execSync('git log --oneline', { cwd: config.resolved.srcDir, encoding: 'utf8' });
    const gitPassed = [];
    const matches = log.matchAll(/phase(\d+)\/passed/g);
    for (const m of matches) {
      const phaseNum = parseInt(m[1]);
      if (!gitPassed.includes(phaseNum)) gitPassed.push(phaseNum);
    }
    state.phases_completed = gitPassed.sort((a, b) => a - b);
  } catch (e) {}
  return state;
}

function resolveNextAction(config, state) {
  if (!fs.existsSync(config.resolved.specsDir)) return { action: 'done', phase: null, reason: 'no_specs' };
  
  const specs = fs.readdirSync(config.resolved.specsDir)
    .filter(f => f.match(/^phase(\d+)\.md$/))
    .map(f => parseInt(f.match(/\d+/)[0]))
    .sort((a, b) => a - b);

  if (specs.length === 0) return { action: 'done', phase: null, reason: 'no_specs' };
  
  if (state.current && state.current.status !== STATUS.COMPLETED) {
    return { action: 'resume', phase: state.current.phase, status: state.current.status };
  }

  const next = specs.find(n => !state.phases_completed.includes(n));
  return next ? { action: 'run', phase: next } : { action: 'done', phase: null, reason: 'all_complete' };
}

module.exports = { 
  STATUS, 
  read, 
  write, 
  syncWithGit, 
  resolveNextAction, 
  startPhase: (config, phase) => { 
    const s = read(config); 
    s.current = { phase, verifier_iteration: 1, status: STATUS.CLEAN }; 
    write(config, s); 
  },
  updateProgress: (config, { verifierIteration, status }) => { 
    const s = read(config); 
    if (s.current) { 
      if (verifierIteration) s.current.verifier_iteration = verifierIteration; 
      if (status) s.current.status = status; 
      write(config, s); 
    } 
  },
  completePhase: (config, phase) => { 
    const s = read(config); 
    if (!s.phases_completed.includes(phase)) s.phases_completed.push(phase); 
    s.current = null; 
    write(config, s); 
  },
  resetPhase: (config, phase) => { 
    const s = read(config); 
    s.phases_completed = s.phases_completed.filter(n => n !== phase); 
    s.current = null; 
    write(config, s); 
  }
};
--- ./src/lib/loop.js ---
'use strict';

const log = require('./logger');
const git = require('./git');
const executor = require('./executor');
const tokens = require('./tokens');
const state = require('./state');
const fs = require('fs');
const path = require('path');

async function runPhase(config, phase, opts = {}) {
  const maxVi = config.loop.max_verifier_iterations;
  const bModel = opts.builderModel || config.models.builder;
  const vModel = opts.verifierModel || config.models.verifier;

  state.startPhase(config, phase);
  git.ensureRepo(config.resolved.srcDir);
  git.ensureRepo(config.resolved.testsDir);

  for (let vi = 1; vi <= maxVi; vi++) {
    log.phase(vi, maxVi);
    
    // BUILDER
    state.updateProgress(config, { verifierIteration: vi, status: state.STATUS.BUILDING });
    log.info('Builder running...');
    try {
      const bLog = await executor.runBuilder(config, phase, { iteration: vi, model: bModel });
      tokens.extractTokens(config, bLog, 'builder', phase, vi);
      git.commitSrc(config, phase, vi, 'build');
      log.success('Builder done');
    } catch (e) {
      log.fail(`Builder crashed: ${e.message}`);
      state.updateProgress(config, { status: state.STATUS.STUCK });
      return;
    }

    // VERIFIER
    state.updateProgress(config, { status: state.STATUS.VERIFYING });
    log.info('Verifier running...');
    try {
      const vLog = await executor.runVerifier(config, phase, { iteration: vi, model: vModel });
      tokens.extractTokens(config, vLog, 'verifier', phase, vi);
      git.commitTests(config, phase, vi);
      log.success('Verifier done');
    } catch (e) {
      log.fail(`Verifier crashed: ${e.message}`);
      state.updateProgress(config, { status: state.STATUS.STUCK });
      return;
    }

    // CHECK FAILURES
    const failPath = path.join(config.resolved.testsDir, 'failed_specs.md');
    const hasFailures = fs.existsSync(failPath) && fs.readFileSync(failPath, 'utf8').trim().includes('SPEC:');

    if (!hasFailures) {
      git.commitPass(config, phase);
      state.completePhase(config, phase);
      log.passed(phase);
      return;
    }

    log.contractsFailed(config.resolved.testsDir);
    state.updateProgress(config, { status: state.STATUS.CONTRACTS_FAILED });
  }

  log.stuck('Max iterations reached.', 'Refine your specs or check builder/verifier reports.');
  state.updateProgress(config, { status: state.STATUS.STUCK });
}

module.exports = { runPhase };
--- ./src/lib/git.js ---
'use strict';

const { execSync } = require('child_process');
const fs = require('fs');
const path = require('path');

function ensureRepo(dir) {
  if (!fs.existsSync(path.join(dir, '.git'))) {
    execSync('git init --quiet', { cwd: dir });
    try {
      execSync('git config user.name "ABL"', { cwd: dir });
      execSync('git config user.email "abl@local"', { cwd: dir });
    } catch (e) {}
  }
}

function commit(dir, message, allowEmpty = false) {
  try {
    execSync('git add -A', { cwd: dir, stdio: 'pipe' });
    const diff = execSync('git diff --cached --name-only', { cwd: dir, encoding: 'utf8' });
    if (!diff.trim() && !allowEmpty) return;
    execSync(`git commit -m "${message}" ${allowEmpty ? '--allow-empty' : ''} --quiet`, { cwd: dir });
  } catch (e) {}
}

module.exports = {
  commitSrc: (config, phase, step, suffix) => commit(config.resolved.srcDir, `phase${phase}/build/step-${step}/${suffix}`),
  commitTests: (config, phase, step) => commit(config.resolved.testsDir, `phase${phase}/verify/step-${step}/results`),
  commitPass: (config, phase) => commit(config.resolved.srcDir, `phase${phase}/passed`, true),
  ensureRepo
};
--- ./examples/nextjs/abl.config.yaml ---
directories:
  src: ./src
  tests: ./tests

models:
  builder: gemini-2.5-pro
  verifier: gemini-2.5-flash

commands:
  health_check: npm run lint && npx tsc --noEmit
  start_dev: npm run dev
  reset_state: npm run db:seed
  map_deps: cat package.json

dev_server:
  port: 3000
  ready_endpoint: /api/health
  timeout_ms: 15000

loop:
  max_health_attempts: 10
  max_verifier_iterations: 5

--- ./examples/nextjs/abl.config.sh ---
#!/bin/bash
# abl.config.sh — project-specific configuration for ABL
# Edit this file per project. Never edit the Makefile.

COMMAND=${1}

case "$COMMAND" in

  # ── Path configuration ────────────────────────────────────────────────────
  # Return the source code directory (relative to project root)
  src_dir)
    echo "src"
    ;;

  # Return the tests directory (relative to project root)
  tests_dir)
    echo "tests"
    ;;

  # ── Dev server ────────────────────────────────────────────────────────────
  start_dev)
    mkdir -p logs
    kill $(lsof -ti:3000) 2>/dev/null
    rm -f src/.next/dev/lock
    sleep 1
    cd src
    npm run dev > ../logs/dev.log 2>&1 &
    echo $! > ../logs/dev.pid
    cd ..
    sleep 5
    ;;

  stop_dev)
    kill -- -$(cat logs/dev.pid) 2>/dev/null
    pkill -f "next dev" 2>/dev/null
    kill $(lsof -ti:3000) 2>/dev/null
    rm -f src/.next/dev/lock
    ;;

  # ── Deterministic health checks ───────────────────────────────────────────
  # All output captured to logs/health.log
  # Return non-zero if any check fails
  health_check)
    mkdir -p logs
    > logs/health.log

    cd src

    echo "=== LINT ===" >> ../logs/health.log
    npm run lint >> ../logs/health.log 2>&1
    lint_exit=$?

    echo "=== TYPECHECK ===" >> ../logs/health.log
    npx tsc --noEmit >> ../logs/health.log 2>&1
    tsc_exit=$?

    # Add more checks as needed:
    # echo "=== UNIT TESTS ===" >> ../logs/health.log
    # npm test -- --passWithNoTests >> ../logs/health.log 2>&1
    # test_exit=$?

    cd ..
    [ $lint_exit -eq 0 ] && [ $tsc_exit -eq 0 ]
    ;;

  # ── State reset ───────────────────────────────────────────────────────────
  # Runs before every Verifier iteration
  # No-op if stateless, otherwise reset DB/cache/queues
  reset_state)
    cd src
    npm run db:seed || { echo "  ✗  Seed failed — aborting"; cd ..; exit 1; }
    cd ..
    ;;

  # ── Dependency map ────────────────────────────────────────────────────────
  # Appended to project_map.txt before every LLM call
  map_deps)
    cat src/package.json
    ;;

  *)
    echo "Unknown command: $COMMAND"
    exit 1
    ;;

esac