
--- ./package-lock.json ---
{
  "name": "abl",
  "version": "2.0.0-alpha.1",
  "lockfileVersion": 3,
  "requires": true,
  "packages": {
    "": {
      "name": "abl",
      "version": "2.0.0-alpha.1",
      "license": "MIT",
      "dependencies": {
        "chalk": "^4.1.2",
        "commander": "^12.0.0",
        "js-yaml": "^4.1.0"
      },
      "bin": {
        "abl": "src/index.js"
      },
      "engines": {
        "node": ">=18.0.0"
      }
    },
    "node_modules/ansi-styles": {
      "version": "4.3.0",
      "resolved": "https://registry.npmjs.org/ansi-styles/-/ansi-styles-4.3.0.tgz",
      "integrity": "sha512-zbB9rCJAT1rbjiVDb2hqKFHNYLxgtk8NURxZ3IZwD3F6NtxbXZQCnnSi1Lkx+IDohdPlFp222wVALIheZJQSEg==",
      "license": "MIT",
      "dependencies": {
        "color-convert": "^2.0.1"
      },
      "engines": {
        "node": ">=8"
      },
      "funding": {
        "url": "https://github.com/chalk/ansi-styles?sponsor=1"
      }
    },
    "node_modules/argparse": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/argparse/-/argparse-2.0.1.tgz",
      "integrity": "sha512-8+9WqebbFzpX9OR+Wa6O29asIogeRMzcGtAINdpMHHyAg10f05aSFVBbcEqGf/PXw1EjAZ+q2/bEBg3DvurK3Q==",
      "license": "Python-2.0"
    },
    "node_modules/chalk": {
      "version": "4.1.2",
      "resolved": "https://registry.npmjs.org/chalk/-/chalk-4.1.2.tgz",
      "integrity": "sha512-oKnbhFyRIXpUuez8iBMmyEa4nbj4IOQyuhc/wy9kY7/WVPcwIO9VA668Pu8RkO7+0G76SLROeyw9CpQ061i4mA==",
      "license": "MIT",
      "dependencies": {
        "ansi-styles": "^4.1.0",
        "supports-color": "^7.1.0"
      },
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/chalk/chalk?sponsor=1"
      }
    },
    "node_modules/color-convert": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/color-convert/-/color-convert-2.0.1.tgz",
      "integrity": "sha512-RRECPsj7iu/xb5oKYcsFHSppFNnsj/52OVTRKb4zP5onXwVF3zVmmToNcOfGC+CRDpfK/U584fMg38ZHCaElKQ==",
      "license": "MIT",
      "dependencies": {
        "color-name": "~1.1.4"
      },
      "engines": {
        "node": ">=7.0.0"
      }
    },
    "node_modules/color-name": {
      "version": "1.1.4",
      "resolved": "https://registry.npmjs.org/color-name/-/color-name-1.1.4.tgz",
      "integrity": "sha512-dOy+3AuW3a2wNbZHIuMZpTcgjGuLU/uBL/ubcZF9OXbDo8ff4O8yVp5Bf0efS8uEoYo5q4Fx7dY9OgQGXgAsQA==",
      "license": "MIT"
    },
    "node_modules/commander": {
      "version": "12.1.0",
      "resolved": "https://registry.npmjs.org/commander/-/commander-12.1.0.tgz",
      "integrity": "sha512-Vw8qHK3bZM9y/P10u3Vib8o/DdkvA2OtPtZvD871QKjy74Wj1WSKFILMPRPSdUSx5RFK1arlJzEtA4PkFgnbuA==",
      "license": "MIT",
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/has-flag": {
      "version": "4.0.0",
      "resolved": "https://registry.npmjs.org/has-flag/-/has-flag-4.0.0.tgz",
      "integrity": "sha512-EykJT/Q1KjTWctppgIAgfSO0tKVuZUjhgMr17kqTumMl6Afv3EISleU7qZUzoXDFTAHTDC4NOoG/ZxU3EvlMPQ==",
      "license": "MIT",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/js-yaml": {
      "version": "4.1.1",
      "resolved": "https://registry.npmjs.org/js-yaml/-/js-yaml-4.1.1.tgz",
      "integrity": "sha512-qQKT4zQxXl8lLwBtHMWwaTcGfFOZviOJet3Oy/xmGk2gZH677CJM9EvtfdSkgWcATZhj/55JZ0rmy3myCT5lsA==",
      "license": "MIT",
      "dependencies": {
        "argparse": "^2.0.1"
      },
      "bin": {
        "js-yaml": "bin/js-yaml.js"
      }
    },
    "node_modules/supports-color": {
      "version": "7.2.0",
      "resolved": "https://registry.npmjs.org/supports-color/-/supports-color-7.2.0.tgz",
      "integrity": "sha512-qpCAvRl9stuOHveKsn7HncJRvv501qIacKzQlO/+Lwxc9+0q2wLyv4Dfvt80/DPn2pqOBsJdDiogXGR9+OvwRw==",
      "license": "MIT",
      "dependencies": {
        "has-flag": "^4.0.0"
      },
      "engines": {
        "node": ">=8"
      }
    }
  }
}

--- ./prompts/verifier.md ---
# Verifier

You are an adversarial QA engineer. You are currently testing the implementation of the **Phase** and **Iteration** specified in the "Session State" section of your context. Your job is to prove the implementation is wrong, not to confirm it is right. You test the live running system against the specs. You never read source code.

## Your workspace
- You are executed in the project's tests directory. This is your root.
- You can read and write files freely here.
- The "Current Phase Spec" is in your context.
- To access specs from previous phases (for cumulative testing), use: `abl-cmd get-spec <N>`

## Your context (injected)
- Project description and map
- Session State (Phase/Iteration)
- Current Phase Spec
- Available Commands

## How to work
1. Read the "Current Phase Spec" to understand the scope.
2. If this is a cumulative phase, use `abl-cmd get-spec <N>` to retrieve previous contracts as needed.
3. Check your working directory for existing test scripts — reuse and refine them.
    CRITICAL: 
        You must create and persist at least ONE test script file per phase for record. 
        Every test you ran must be recorded int he test script of the phase.
        The test scripts should be appropriately named.
4. **Prepare Environment:** Before running tests, you must ensure the system is in a clean and ready state. Use the tools provided in "Available Commands" via `abl-cmd` (e.g. seeding).
5. Run tests using bash — curl, playwright, or any appropriate tool.
    CRITICAL RULES FOR BASH SCRIPTS:
      - ALWAYS use http://127.0.0.1 instead of localhost to prevent IPv6 curl timeouts.
      - If splitting curl commands across lines, you MUST use \ to continue the line.
6. Collect real stdout/stderr — never assume or hallucinate results.
7. Be adversarial — explicitly test three dimensions for every contract:
   - **Happy Path:** The exact input specified in the contract.
   - **Bad Path:** Common invalid inputs (e.g., missing fields, wrong types) derived from the Context.
   - **Edge Cases:** Boundary conditions, malformed payloads, injection attempts, zero/null values, and attempts to violate negative constraints (e.g., immutability, access controls) defined in the Context.
   - *Never stop at the Happy Path.* If a contract implies a constraint, you must actively try to break it.
8. Session isolation: each independent test scenario must establish its own fresh session.

## Your sole output signal is failed_specs.md

This file is the only thing that determines pass or fail.

- If ALL contracts pass: delete `failed_specs.md` if it exists.
- If ANY contract fails: write `failed_specs.md` with every failure in the specified format.

Do not report pass or fail in prose. The file presence is the verdict.

## failed_specs.md format

```
SPEC: [exact contract that failed]
INPUT: [exact input/payload/request used]
OBSERVED: [exact response received]

SPEC: [next failure]
INPUT: ...
OBSERVED: ...
```

## Failed Specs File Management
If you run multiple test scripts, aggregate all results into a single
`failed_specs.md` at the end — do not rely on individual script outputs.

## Run Report (mandatory — write this before closing)

Write `verifier_reports/phase{N}_v{I}_{YYYYMMDD_HHMM}.md` in your current directory before closing. Always — pass or fail. Replace `{N}` and `{I}` with the Phase and Iteration numbers from your Session State.

```markdown
# Verifier Run Report
Phase: {N} | Iteration: {I} | {timestamp}

## What I did
- Which specs I reviewed
- Which test files already existed vs what I created fresh
- How many test scripts I ran and in what order
- Total contracts tested / passed / failed

## Friction log
- Every moment you had to probe, retry, or guess to understand system behavior
- Any spec ambiguities that forced assumptions
- Any context that was missing and required extra tool calls to discover
- Any endpoint behaviors that surprised you or required multiple attempts

## Turn self-assessment
- Rough split: how many turns were productive (writing/running tests) vs discovery (figuring out what exists, port, auth flow, response shape)
- What single piece of information, if provided upfront, would have saved the most turns this run?
```

## Rules
- Never read source files — you are a black box tester.
- Never write or modify application code.
- **You are responsible for preparing the environment via `abl-cmd` before testing.**
- Every test must be anchored to a spec contract — no invented expectations.
- **You MUST write the Verifier Run Report before finishing.**
- Your task is complete when failed_specs.md is written or deleted AND run report is written.

CRITICAL: You MUST output a brief `Adversarial Test Plan` block in the test files. In this block, explicitly list:
1. The Bad Paths you derived from the Context constraints.
2. The Edge Cases (malformed data, boundary conditions) you intend to test.
3. How you will verify negative constraints (e.g., immutability, encryption, revocation) described in the Context.
--- ./prompts/builder.md ---
# Builder

You are a senior engineer. You are currently working on the **Phase** and **Iteration** specified in the "Session State" section of your context. Your job is to implement the current phase spec exactly. Nothing more.

## Your workspace
- You are executed in the project's source directory. This is your root.
- You can read and write files freely here.
- The spec for the current phase is provided in the "Current Phase Spec" section of your context.

## Your context (injected)
- Project description and map
- Session State (Phase/Iteration)
- Current Phase Spec
- Available Commands

## Shipping Standards: Zero Tolerance
You are responsible for the technical integrity of the code you ship. A phase is not complete until the implementation is stable and professional.
1. **Zero Errors:** Your code must ship with no runtime errors, no lint errors, and no type errors.
2. **Health Checks:** If the "Available Commands" section includes a health check or validation command, you MUST run it via `abl-cmd` and ensure it passes before finishing your turn.
3. **Significant Warnings:** The code must run without significant warnings. Do not leave "todo" comments or half-finished implementations.
4. **Environment Management:** Use the provided commands as necessary to ensure the system is in a ready state (e.g., migrations, dependency installation).
5. **Project integrity:** Any previous phase have been tested and human audited and approved. You must take care that your current implementation respects already implemented working business logic and code of the previous phases.

## Run Report (mandatory — write this before closing)

Write `builder_reports/phase{N}_v{I}_{YYYYMMDD_HHMM}.md` in your current directory before closing. Always. Replace `{N}` and `{I}` with the Phase and Iteration numbers from your Session State.

```
# Builder Run Report
Phase: {N} | Iteration: {I} | {timestamp}

## What I did
- Which contracts I implemented/fixed
- Which source files I modified or created
- Which `abl-cmd` tools I ran and their results
- Summary of architectural changes made

## Friction log
- Every moment you had to probe, retry, or guess to understand system behavior or existing code
- Any spec ambiguities that forced assumptions
- Any context that was missing and required extra tool calls to discover
- Any command failures that required multiple attempts to resolve

## Turn self-assessment
- Rough split: how many turns were productive (writing code) vs discovery (figuring out what exists, response shapes, dependency mapping)
- What single piece of information, if provided upfront, would have saved the most turns this run?
```

## How to work
1. Read the "Current Phase Spec" in your context.
2. Read existing source files relevant to the spec before writing anything.
3. Implement what the contracts require — exactly, nothing more.
4. If `failed_specs.md` exists in your workspace (or you are informed of failures), fix those contracts first.
5. Use your project commands via `abl-cmd` to verify your work. If a health check is available, you must pass it. Capture and read any command output to identify and resolve issues.

## Rules
- Specs are law — implement exactly what they say.
- **You MUST ensure the implementation passes all provided health checks.**
- **You MUST write the Builder Run Report before finishing.**
- Do NOT run adversarial tests (that is the Verifier's job).
- Do NOT modify anything outside your working directory.
- When your code is stable, error-free, and passes health checks, stop.
--- ./docs/autonomous_build_loop.md ---
# Autonomous Build Loop (ABL)
## Concept Formalization v0.8

---

## Problem Statement

LLM-assisted development workflows that mirror human sprint team structures are inefficient because the orchestration burden falls on the human, role-playing generates fictional artifacts, and token usage is spent on organizational theater rather than intelligence. ABL eliminates all of this.

---

## Core Concept

A phase-based autonomous build loop where the human writes specs and performs final audit. Everything in between is automatic. The orchestration is handled by a Node.js CLI. The LLM engine is Gemini CLI.

---

## Mental Model

```
Specs → [abl phase N] → Verified output → Human audit → approve or refine specs → next phase
```

The human touches the system exactly twice per phase: spec input and audit output.

---

## Roles

**Builder** — reads the dynamically injected phase and iteration, pulls the current phase spec from `specs/`, reads the codebase. Implements contracts exactly. If `failed_specs.md` exists, fixes those first. Manages its own technical quality using project-defined `abl-cmd` tools (lint, typecheck, migrations). Produces a Builder Run Report before finishing.

**Verifier** — reads the dynamically injected phase and iteration, pulls all cumulative specs from `specs/`, tests the live running system. Prepares the test environment using project-defined `abl-cmd` tools (seeding, starting server). Reports failures via `failed_specs.md`. Never reads source code. Produces a Verifier Run Report before finishing.

There is no Rectifier. The Builder is the only entity that touches code.

---

## Information Boundaries

| Role | Reads | Never reads |
|---|---|---|
| Builder | specs/ + SRC_DIR codebase + failed_specs.md | TESTS_DIR, prompts/verifier.md |
| Verifier | specs/ + TESTS_DIR + failed_specs.md | SRC_DIR |

Boundaries are enforced by **Docker**. Each role runs in an isolated container. `specs/` is mounted read-only for both roles.

---

## Communication Channel: Builder ↔ Verifier

The Verifier passes failures to the Builder via `TESTS_DIR/failed_specs.md`. Each entry contains the failed contract, the exact input used, and the observed system behavior. Written entirely in spec language — no test code, no test output, no implementation details.

Format:

```
SPEC: POST /auth/login {valid credentials} → 200 {token}
INPUT: {"email": "test@test.com", "password": "correct"}
OBSERVED: 404

SPEC: GET /dashboard (valid token) → 200
INPUT: GET /dashboard headers: {Authorization: Bearer <token>}
OBSERVED: redirect /login
```

INPUT is mandatory — the Builder must know exactly what stimulus triggered the failure.

**Pass/fail signal:** The presence of `failed_specs.md` containing at least one `SPEC:` entry means failure. Absence or empty file means pass.

---

## Git Strategy

Two separate git repositories — one in SRC_DIR, one in TESTS_DIR. The Docker boundary extends into version control. Neither role can traverse the other's git history.

---

## File Structure

```
project/
├── [SRC_DIR]/                # configurable — default "src"
│   ├── .git/
│   ├── [all application code]
│   └── builder_reports/      # Builder's iteration reports
├── .abl/
│   ├── tests/                # Verifier's workspace (configurable)
│   │   ├── .git/
│   │   ├── failed_specs.md   # Written by Verifier on failure, deleted on pass
│   │   └── verifier_reports/ # Verifier's iteration reports
│   ├── specs/
│   │   ├── phase1.md
│   │   └── phaseN.md
│   ├── logs/                 # Runtime artifacts
│   │   ├── builder.log       # Full Gemini CLI JSON output from Builder (last run)
│   │   ├── verifier.log      # Full Gemini CLI JSON output from Verifier (last run)
│   │   └── tokens.csv        # Cumulative token usage across all phases
│   ├── project.md            # Human-written project description
│   └── project_map.txt       # Auto-generated before every LLM call
├── .env                      # Secrets — gitignored, never passed to LLMs
└── prompts/
    ├── builder.md
    └── verifier.md
```

---

## The Loop

```
abl phase PHASE=N:
    - Initialize SRC_DIR/.git and TESTS_DIR/.git if not present
    - Initialize .abl/logs/tokens.csv
    - Inject <<PHASE>> and <<ITERATION>> into prompts

outer loop (max 5 Verifier iterations):

    Builder runs in Docker (SRC_DIR + specs/)
    Builder implements specs
    Builder runs self-checks via abl-cmd
    Builder writes report to src/builder_reports/
    git commit SRC_DIR: "phaseN/build/step-X"

    Verifier runs in Docker (tests/ + specs/)
    Verifier prepares environment via abl-cmd
    Verifier writes and runs tests
    Verifier writes report to tests/verifier_reports/
    → failed_specs.md absent or empty: ✓ PASS — surface to human
    → failed_specs.md has SPEC entries: FAIL
      git commit TESTS_DIR: "phaseN/verify/step-X"
      next outer iteration

outer loop exhausted (5 Verifier fails): STUCK (contracts) → surface failed_specs.md
```

---

## Continuous Phases

`abl phase N` is idempotent and resumable. The loop always picks up from where it left off based on the internal `state.json`.

---

## Token Tracking

Every Gemini call appends a row to `.abl/logs/tokens.csv`. View cumulative totals:
```bash
abl costs
```

---

## Spec and Phase Requirements

### What a Spec Is

A spec is a behavioral contract. It defines a specific action and a specific expected result with no room for interpretation. The Verifier must be able to derive a test from it without any implementation knowledge.

### Spec Format

**Context** — natural language intent, architecture decisions, constraints.

**Contracts** — exact quasi-code action → result pairs. One action, one result, no ambiguity.

```
ACTION → EXPECTED RESULT
```

### Spec Rules

- First line must be `# Phase N: Title`
- Specs are append-only across phases
- Cumulative: the Verifier tests ALL phases every iteration, not just the current one

---

## Environment and Security

**Docker** is the enforcement layer. Builder sees only SRC_DIR and specs/. Verifier sees only TESTS_DIR and specs/.

**Secrets** live in `.env` at project root, gitignored. The runner reads `GEMINI_API_KEY` from `.env` and passes it as an environment variable to the container.

---

## Human Audit

The human audits the phase output as a complete experience. Problems are addressed by refining specs — never by patching code directly. All corrections become permanent spec knowledge.

--- ./package.json ---
{
  "name": "abl",
  "version": "2.0.0-alpha.1",
  "description": "Autonomous Build Loop — a phase-based AI development framework",
  "main": "src/index.js",
  "bin": {
    "abl": "./src/index.js"
  },
  "scripts": {
    "start": "node src/index.js"
  },
  "dependencies": {
    "chalk": "^4.1.2",
    "commander": "^12.0.0",
    "js-yaml": "^4.1.0"
  },
  "engines": {
    "node": ">=18.0.0"
  },
  "keywords": [
    "ai",
    "autonomous",
    "build",
    "loop",
    "gemini",
    "llm",
    "framework",
    "tdd"
  ],
  "license": "MIT"
}

--- ./README.md ---
# ABL — Autonomous Build Loop

ABL is a phase-based autonomous development framework. It orchestrates two isolated AI roles—a **Builder** and a **Verifier**—to implement software iteratively based on strict behavioral contracts.

## How it Works

1.  **Human Writes Specs**: You define a phase as a list of behavioral "contracts" (Action → Result).
2.  **Builder Implements**: The Builder reads the spec and writes the code.
3.  **Verifier Tests**: The Verifier attempts to break the implementation by testing it against the contracts.
4.  **Looping**: If the Verifier finds a failure, it writes a `failed_specs.md` file. The Builder receives this file and tries again.
5.  **Audit**: Once all contracts pass, the loop finishes, and you audit the result.

## Prerequisites

-   **Node.js**: >= 18.0.0
-   **Gemini CLI**: `npm install -g @google/generative-ai-cli`
-   **API Key**: A valid `GEMINI_API_KEY` in your project's `.env` file.
-   **Git**: Required for the internal state tracking and versioning.

## Installation

```bash
npm install -g abl
```

## Quick Start

1.  **Initialize**:
    ```bash
    mkdir my-project && cd my-project
    abl init
    ```
2.  **Define Project**: Edit `.abl/project.md` to describe the tech stack and goals.
3.  **Write Phase 1**: Edit `.abl/specs/phase1.md` with your first contracts.
4.  **Run**:
    ```bash
    abl run
    ```

---

## Writing High-Quality Specs

Specs are behavioral contracts. They must be **non-interpretable**. If a human tester wouldn't know exactly what to do, the AI won't either.

### The Contract Format
A contract consists of a stimulus and an expected response.

**Bad Spec (Vague):**
> "The login page should work and show an error if the password is wrong."

**Good Spec (Contract):**
> `POST /api/login {"email": "user@test.com", "pass": "wrong"} → 401 Unauthorized`

### Spec Rules
1.  **Actionable**: Every contract must be testable via a command or script.
2.  **Atomic**: One action, one result.
3.  **Cumulative**: The Verifier tests all previous phases in every run to prevent regressions.

---

## Configuration (`abl.config.yaml`)

This file defines the workspace and how the agents interact with your environment.

```yaml
directories:
  src: ./src        # Where the Builder works
  tests: ./tests    # Where the Verifier works

models:
  builder: gemini-2.0-pro
  verifier: gemini-2.0-flash

builder_commands:
  health_check: 
    command: "npm run lint && npx tsc --noEmit"
    description: "Check for lint and type errors"

verifier_commands:
  seed: 
    command: "npm run db:seed"
    description: "Reset and seed the database"
  start_dev: 
    command: "npm run dev &"
    description: "Start the server in background"
  stop_dev:
    command: "pkill -f next-server"
    description: "Kill the dev server"
```

### `abl-cmd`
Agents do not see your raw shell commands. They see the names (e.g., `health_check`) and descriptions. They execute them using `abl-cmd <name>`.

---

## Command Reference

-   **`abl init`**: Sets up the `.abl` directory, role prompts, and initializes git in your `src` and `tests` folders.
-   **`abl run`**: Resumes the current phase or starts the next pending phase.
-   **`abl phase <N>`**: Forces the execution of a specific phase.
-   **`abl costs`**: Summarizes token usage and costs from `.abl/logs/tokens.csv`.

---

## Recommended Practices

### What to use ABL for:
-   **API Backends**: Perfectly suited for request/response contracts.
-   **CLI Tools**: Easy to define input/output expectations.
-   **Logic Modules**: Great for data processing or complex algorithms.

### What NOT to use ABL for:
-   **UI Polish**: "Make the button look nice" is not a contract.
-   **Exploratory Coding**: If you don't know the architecture yet, ABL will struggle.
-   **Large Refactors**: ABL is phase-based; large architectural shifts should be defined in a new phase's context.

### Tips for Success:
-   **Use Health Checks**: Always provide the Builder with a `health_check` command (linting/typechecking).
-   **Isolated Iterations**: The Verifier should always run a `seed` or `reset` command before testing to ensure a clean state.
-   **Granular Phases**: Keep phases small (5-10 contracts). Large phases increase the chance of the Builder getting stuck in a "fix one, break another" loop.

## Project Structure

```text
.
├── .abl/
│   ├── abl.config.yaml   # Configuration
│   ├── project.md        # High-level project context
│   ├── state.json        # Current progress tracking
│   ├── specs/            # Phase definitions (phase1.md, etc)
│   ├── prompts/          # System prompts for roles (do not edit unless advanced)
│   └── logs/             # Role logs and token usage
├── src/                  # Builder's workspace (App code)
├── tests/                # Verifier's workspace (Test scripts/logs)
└── .env                  # GEMINI_API_KEY
```
--- ./v1/Makefile ---
PHASE          ?= 1
BUILDER_MODEL  ?= gemini-3-flash-preview
VERIFIER_MODEL ?= gemini-3-flash-preview
SPECS          := $(wildcard specs/phase*.md)
GEMINI         := $(HOME)/.nvm/versions/node/v22.21.0/bin/gemini
FIREJAIL       := firejail --noprofile \
                  --whitelist=$(HOME)/.gemini \
                  --whitelist=$(HOME)/.nvm \
                  --whitelist=$(shell pwd)/.abl
SRC_DIR        := $(shell bash abl.config.sh src_dir)
TESTS_DIR      := $(shell bash abl.config.sh tests_dir)
GEMINI_API_KEY := $(shell grep ^GEMINI_API_KEY .env 2>/dev/null | cut -d= -f2)
LEAN_CONFIG    := $(shell pwd)/.abl/lean_settings.json

# ── Helpers ────────────────────────────────────────────────────────────────

_setup:
	@mkdir -p .abl logs specs
	@sed -i "s|ABSOLUTE_PATH_PLACEHOLDER|$(shell pwd)|g" .abl/lean_settings.json 2>/dev/null || true

_extract_tokens:
	@log=logs/$(ROLE).log; \
	ts=$$(date '+%Y-%m-%d %H:%M:%S'); \
	json=$$(echo "{"; awk '/"stats":/{f=1} f; /^}/{if(f) exit}' "$$log"); \
	session=$$(grep -o '"session_id": *"[^"]*"' "$$log" | head -1 | grep -o '"[^"]*"$$' | tr -d '"'); \
	if echo "$$json" | jq -e '.stats' > /dev/null 2>&1; then \
		echo "$$json" | jq -r --arg ts "$$ts" --arg phase "$(PHASE)" --arg step "$(STEP)" \
		--arg role "$(ROLE)" --arg session "$$session" \
		'.stats.models | to_entries[] | [$$ts, $$phase, $$step, $$role, .key, (.value.tokens.input // 0), (.value.tokens.candidates // 0), (.value.tokens.cached // 0), (.value.tokens.total // 0), $$session] | @csv' \
		>> logs/tokens.csv; \
	else \
		echo "\"$$ts\",\"$(PHASE)\",\"$(STEP)\",\"$(ROLE)\",\"unknown\",\"ERROR\",0,0,0,0,\"none\"" >> logs/tokens.csv; \
	fi

_last_session:
	@tail -1 logs/tokens.csv | cut -d',' -f10 | tr -d '"'

_map:
	@tree $(SRC_DIR)/ -I 'node_modules|.git' --dirsfirst > project_map.txt
	@echo "---" >> project_map.txt
	@bash abl.config.sh map_deps >> project_map.txt

_index:
	@head -1 specs/phase$(PHASE).md >> specs/index.md

# ── Roles ──────────────────────────────────────────────────────────────────

_build:
	@$(MAKE) --no-print-directory _map
	@echo "  ⚙  Builder running..."
	@cat prompts/builder.md project.md project_map.txt specs/index.md \
	| $(FIREJAIL) --whitelist=$(shell pwd)/$(SRC_DIR) --whitelist=$(shell pwd)/specs \
	  env GEMINI_API_KEY=$(GEMINI_API_KEY) \
	  GEMINI_CLI_SYSTEM_SETTINGS_PATH=$(LEAN_CONFIG) \
	  $(GEMINI) -m $(BUILDER_MODEL) \
	  --include-directories $(shell pwd)/$(SRC_DIR),$(shell pwd)/specs \
	  -y --output-format json \
	  -p "Execute your build instructions for phase $(PHASE)." \
	  > logs/builder.log 2>&1
	@$(MAKE) --no-print-directory _extract_tokens ROLE=builder STEP=$(STEP)
	@cd $(SRC_DIR) && git add -A && { git diff --cached --quiet || git commit -m "phase$(PHASE)/build/step-$(STEP)/pre-deterministic" --quiet; } && cd ..
	@echo "  ✓  Builder done"


_build_with_context:
	@$(MAKE) --no-print-directory _map
	@echo "  ⚙  Builder running with failure context..."
	@cat prompts/builder.md project.md project_map.txt specs/index.md \
	  $(shell [ -f $(TESTS_DIR)/failed_specs.md ] && echo $(TESTS_DIR)/failed_specs.md) \
	  $(shell [ -f logs/health.log ] && echo logs/health.log) \
	| $(FIREJAIL) --whitelist=$(shell pwd)/$(SRC_DIR) --whitelist=$(shell pwd)/specs \
	  env GEMINI_API_KEY=$(GEMINI_API_KEY) \
	  GEMINI_CLI_SYSTEM_SETTINGS_PATH=$(LEAN_CONFIG) \
	  $(GEMINI) -m $(BUILDER_MODEL) \
	  --include-directories $(shell pwd)/$(SRC_DIR),$(shell pwd)/specs \
	  -y --output-format json \
	  -p "Execute your build instructions for phase $(PHASE)." \
	  > logs/builder.log 2>&1
	@$(MAKE) --no-print-directory _extract_tokens ROLE=builder STEP=$(STEP)
	@cd $(SRC_DIR) && git add -A && { git diff --cached --quiet || git commit -m "phase$(PHASE)/build/step-$(STEP)/pre-deterministic" --quiet; } && cd ..
	@echo "  ✓  Builder done"

_verify:
	@$(MAKE) --no-print-directory _map
	@echo "  ⚙  Verifier running..."
	@cat prompts/verifier.md project.md project_map.txt specs/index.md \
	  $(shell [ -f $(TESTS_DIR)/failed_specs.md ] && echo $(TESTS_DIR)/failed_specs.md) \
	| $(FIREJAIL) --whitelist=$(shell pwd)/$(TESTS_DIR) --whitelist=$(shell pwd)/specs \
	  env GEMINI_API_KEY=$(GEMINI_API_KEY) \
	  GEMINI_CLI_SYSTEM_SETTINGS_PATH=$(LEAN_CONFIG) \
	  $(GEMINI) -m $(VERIFIER_MODEL) \
	  --include-directories $(shell pwd)/$(TESTS_DIR),$(shell pwd)/specs \
	  -y --output-format json \
	  -p "Run your test suite for phase $(PHASE). Write failed_specs.md on failure, delete it on pass." \
	  > logs/verifier.log 2>&1
	@$(MAKE) --no-print-directory _extract_tokens ROLE=verifier STEP=$(STEP)
	@cd $(TESTS_DIR) && git add -A && { git diff --cached --quiet || git commit -m "phase$(PHASE)/verify/step-$(STEP)/results" --quiet; } && cd ..
	@echo "  ✓  Verifier done"

# ── Main loop ──────────────────────────────────────────────────────────────

costs:
	@if [ ! -f logs/tokens.csv ]; then echo "No token data yet — run a phase first."; exit 0; fi
	@echo ""
	@echo "━━━ Token Usage Summary ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
	@awk -F',' 'NR>1 && $$6!="0" { \
		gsub(/"/, "", $$4); gsub(/"/, "", $$5); \
		input+=$$6; candidates+=$$7; cached+=$$8; total+=$$9; calls++ \
	} END { \
		printf "  Total calls:      %d\n", calls; \
		printf "  Input tokens:     %d\n", input; \
		printf "  Output tokens:    %d\n", candidates; \
		printf "  Cached tokens:    %d\n", cached; \
		printf "  Total tokens:     %d\n", total; \
	}' logs/tokens.csv
	@echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
	@echo "  (Apply current model pricing to token counts above)"
	@echo ""

phase:
	@[ -d $(SRC_DIR)/.git ]   || git -C $(SRC_DIR) init --quiet
	@[ -d $(TESTS_DIR)/.git ] || git -C $(TESTS_DIR) init --quiet
	@$(MAKE) --no-print-directory _setup
	@[ -f logs/tokens.csv ] || echo "timestamp,phase,step,role,model,input,candidates,cached,total,session_id" > logs/tokens.csv
	@touch specs/index.md
	@$(MAKE) --no-print-directory _index
	@if [ -f logs/health.log ] || [ -f $(TESTS_DIR)/failed_specs.md ]; then \
		echo ""; \
		echo "↺  Resuming phase $(PHASE) from prior state..."; \
		[ -f logs/health.log ]              && echo "   → health.log found — Builder will receive health errors"; \
		[ -f $(TESTS_DIR)/failed_specs.md ] && echo "   → failed_specs.md found — Builder will receive contract failures"; \
	fi
	@for vi in 1 2 3 4 5; do \
		echo ""; \
		echo "━━━ Verifier iteration $$vi / 5 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"; \
		health_passed=0; \
		echo "  ⚙  Resetting DB state..."; \
		bash abl.config.sh reset_state || { echo "  ✗  DB reset failed — aborting"; exit 1; }; \
		echo "  ✓  DB ready"; \
		for hi in 1 2 3 4 5 6 7 8 9 10; do \
			echo "  ── Build / health attempt $$hi / 10"; \
			if [ $$vi -eq 1 ] && [ $$hi -eq 1 ] && [ ! -f logs/health.log ] && [ ! -f $(TESTS_DIR)/failed_specs.md ]; then \
				$(MAKE) --no-print-directory _build STEP=$$vi || exit 1; \
			else \
				$(MAKE) --no-print-directory _build_with_context STEP=$$vi || exit 1; \
			fi; \
			echo "  ⚙  Health check..."; \
			if bash abl.config.sh health_check; then \
				echo "  ✓  Health check passed"; \
				rm -f logs/health.log; \
				cd $(SRC_DIR) && git add -A && \
					{ git diff --cached --quiet || git commit -m "phase$(PHASE)/build/step-$$vi/post-deterministic" --quiet; } && cd ..; \
				health_passed=1; \
				break; \
			else \
				echo "  ✗  Health check failed (attempt $$hi) — retrying..."; \
			fi; \
		done; \
		if [ $$health_passed -eq 0 ]; then \
			echo ""; \
			echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"; \
			echo "✗  STUCK — Builder could not pass health check after 10 attempts"; \
			echo "    → see logs/health.log"; \
			echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"; \
			exit 1; \
		fi; \
		echo "  ⚙  Resetting DB state for Verifier..."; \
		bash abl.config.sh reset_state || { echo "  ✗  DB reset failed — aborting"; exit 1; }; \
		echo "  ✓  DB ready"; \
		echo "  ⚙  Starting dev server..."; \
		bash abl.config.sh start_dev; \
		echo "  ✓  Dev server ready"; \
		$(MAKE) --no-print-directory _verify STEP=$$vi; \
		bash abl.config.sh stop_dev; \
		echo "  ✓  Dev server stopped"; \
		if [ ! -f $(TESTS_DIR)/failed_specs.md ] || ! grep -q "SPEC:" $(TESTS_DIR)/failed_specs.md; then \
			echo ""; \
			echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"; \
			echo "✓  Phase $(PHASE) passed"; \
			echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"; \
			exit 0; \
		fi; \
		echo "  ✗  Contracts failed — see $(TESTS_DIR)/failed_specs.md"; \
	done; \
	bash abl.config.sh stop_dev 2>/dev/null; \
	echo ""; \
	echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"; \
	echo "✗  STUCK — Builder could not satisfy contracts after 5 Verifier iterations"; \
	echo "    → see $(TESTS_DIR)/failed_specs.md"; \
	echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"; \
	exit 1
--- ./v1/abl.config.sh ---
#!/bin/bash
# abl.config.sh — project-specific configuration for ABL
# Edit this file per project. Never edit the Makefile.

COMMAND=${1}

case "$COMMAND" in

  # ── Path configuration ────────────────────────────────────────────────────
  # Return the source code directory (relative to project root)
  src_dir)
    echo "src"
    ;;

  # Return the tests directory (relative to project root)
  tests_dir)
    echo "tests"
    ;;

  # ── Dev server ────────────────────────────────────────────────────────────
  start_dev)
    mkdir -p logs
    kill $(lsof -ti:3000) 2>/dev/null
    rm -f src/.next/dev/lock
    sleep 1
    cd src
    npm run dev > ../logs/dev.log 2>&1 &
    echo $! > ../logs/dev.pid
    cd ..
    sleep 5
    ;;

  stop_dev)
    kill -- -$(cat logs/dev.pid) 2>/dev/null
    pkill -f "next dev" 2>/dev/null
    kill $(lsof -ti:3000) 2>/dev/null
    rm -f src/.next/dev/lock
    ;;

  # ── Deterministic health checks ───────────────────────────────────────────
  # All output captured to logs/health.log
  # Return non-zero if any check fails
  health_check)
    mkdir -p logs
    > logs/health.log

    cd src

    echo "=== LINT ===" >> ../logs/health.log
    npm run lint >> ../logs/health.log 2>&1
    lint_exit=$?

    echo "=== TYPECHECK ===" >> ../logs/health.log
    npx tsc --noEmit >> ../logs/health.log 2>&1
    tsc_exit=$?

    # Add more checks as needed:
    # echo "=== UNIT TESTS ===" >> ../logs/health.log
    # npm test -- --passWithNoTests >> ../logs/health.log 2>&1
    # test_exit=$?

    cd ..
    [ $lint_exit -eq 0 ] && [ $tsc_exit -eq 0 ]
    ;;

  # ── State reset ───────────────────────────────────────────────────────────
  # Runs before every Verifier iteration
  # No-op if stateless, otherwise reset DB/cache/queues
  reset_state)
    cd src
    npm run db:seed || { echo "  ✗  Seed failed — aborting"; cd ..; exit 1; }
    cd ..
    ;;

  # ── Dependency map ────────────────────────────────────────────────────────
  # Appended to project_map.txt before every LLM call
  map_deps)
    cat src/package.json
    ;;

  *)
    echo "Unknown command: $COMMAND"
    exit 1
    ;;

esac
--- ./src/index.js ---
#!/usr/bin/env node

'use strict';

const { program } = require('commander');
const { version } = require('../package.json');

const runCommand     = require('./commands/run');
const phaseCommand   = require('./commands/phase');
const initCommand    = require('./commands/init');
const costsCommand   = require('./commands/costs');

program
  .name('abl')
  .description('Autonomous Build Loop — a phase-based AI development framework')
  .version(version);

program
  .command('init')
  .description('Initialize a new ABL project in the current directory')
  .action(initCommand);

program
  .command('run')
  .description('Run the next pending phase, or resume an interrupted one')
  .option('-b, --builder-model <model>',  'Override builder model')
  .option('-v, --verifier-model <model>', 'Override verifier model')
  .option('--cwd <path>', 'Run as if from this directory')
  .option('-i, --interactive', 'Run Gemini CLI in interactive mode')
  .action(runCommand);

program
  .command('phase <n>')
  .description('Run a specific phase by number')
  .option('-b, --builder-model <model>',  'Override builder model')
  .option('-v, --verifier-model <model>', 'Override verifier model')
  .option('--cwd <path>', 'Run as if from this directory')
  .option('-i, --interactive', 'Run Gemini CLI in interactive mode')
  .action(phaseCommand);

program
  .command('costs')
  .description('Show cumulative token usage summary')
  .option('--cwd <path>', 'Run as if from this directory')
  .action(costsCommand);

program.parse(process.argv);
--- ./src/commands/run.js ---
'use strict';

const chalk        = require('chalk');
const { loadConfig }        = require('../lib/config');
const { runPhase }          = require('../lib/loop');
const state                 = require('../lib/state');

async function runCommand(opts) {
  if (process.env.ABL_DEBUG) console.log(chalk.dim('DEBUG: runCommand started.'));
  let config;
  try {
    config = loadConfig(opts.cwd);
    if (process.env.ABL_DEBUG) console.log(chalk.dim(`DEBUG: Config loaded from ${config.resolved.root}`));
  } catch (e) {
    console.error(chalk.red(`Error: ${e.message}`));
    process.exit(1);
  }

  // Sync state with git before deciding what to do
  let s = state.read(config);
  if (process.env.ABL_DEBUG) console.log(chalk.dim('DEBUG: Current state read.'));
  s = state.syncWithGit(config, s);
  if (process.env.ABL_DEBUG) console.log(chalk.dim('DEBUG: State synced with git.'));
  state.write(config, s);

  // Resolve what to do next
  const next = state.resolveNextAction(config, s);
  if (process.env.ABL_DEBUG) console.log(chalk.dim(`DEBUG: Next action resolved: ${JSON.stringify(next)}`));

  if (next.action === 'done') {
    if (next.reason === 'no_specs') {
      console.log('');
      console.log(chalk.yellow('! No spec files found.'));
      console.log(chalk.dim(`  Add your first spec to ${config.resolved.specsDir}/phase1.md`));
      console.log('');
    } else {
      console.log('');
      console.log(chalk.green('━'.repeat(57)));
      console.log(chalk.green('✓  All phases complete. Nothing to do.'));
      console.log(chalk.dim(`   Completed: phases ${s.phases_completed.join(', ')}`));
      console.log(chalk.green('━'.repeat(57)));
      console.log('');
    }
    return;
  }

  const { phase } = next;

  if (next.action === 'resume') {
    console.log('');
    console.log(chalk.yellow(`↺  Resuming phase ${phase} (${next.status})...`));
  } else {
    console.log('');
    console.log(chalk.bold(`▶  Running phase ${phase}...`));
  }

  // Apply model overrides
  if (opts.builderModel)  config.models.builder  = opts.builderModel;
  if (opts.verifierModel) config.models.verifier = opts.verifierModel;

  try {
    await runPhase(config, phase, {
      builderModel:  config.models.builder,
      verifierModel: config.models.verifier,
      interactive:   opts.interactive,
    });
  } catch (e) {
    console.error(chalk.red(`\nFatal: ${e.message}`));
    if (process.env.ABL_DEBUG) console.error(e.stack);
    process.exit(1);
  }

  // Phase completed — check if more phases to run
  const updated  = state.syncWithGit(config, state.read(config));
  const nextNext = state.resolveNextAction(config, updated);

  if (nextNext.action === 'run') {
    console.log('');
    console.log(chalk.dim(`  Phase ${phase} done. Run ${chalk.bold('abl run')} again to continue with phase ${nextNext.phase}.`));
    console.log('');
  } else if (nextNext.action === 'done' && nextNext.reason === 'all_complete') {
    console.log('');
    console.log(chalk.green('━'.repeat(57)));
    console.log(chalk.green('✓  All phases complete.'));
    console.log(chalk.green('━'.repeat(57)));
    console.log('');
  }
}

module.exports = runCommand;
--- ./src/commands/costs.js ---
'use strict';

const { loadConfig } = require('../lib/config');
const { printCosts } = require('../lib/tokens');
const chalk          = require('chalk');

function costsCommand(opts) {
  let config;
  try {
    config = loadConfig(opts.cwd);
  } catch (e) {
    console.error(chalk.red(`Error: ${e.message}`));
    process.exit(1);
  }

  printCosts(config);
}

module.exports = costsCommand;
--- ./src/commands/phase.js ---
'use strict';

const chalk          = require('chalk');
const { loadConfig } = require('../lib/config');
const { runPhase }   = require('../lib/loop');
const state          = require('../lib/state');

async function phaseCommand(n, opts) {
  const phase = parseInt(n);
  if (isNaN(phase) || phase < 1) {
    console.error(chalk.red('Error: phase must be a positive integer'));
    process.exit(1);
  }

  let config;
  try {
    config = loadConfig(opts.cwd);
  } catch (e) {
    console.error(chalk.red(`Error: ${e.message}`));
    process.exit(1);
  }

  // Sync state with git
  let s = state.read(config);
  s = state.syncWithGit(config, s);
  state.write(config, s);

  // Warn if a different phase is in progress
  if (s.current && s.current.phase !== phase &&
      s.current.status !== state.STATUS.COMPLETED &&
      s.current.status !== state.STATUS.STUCK) {
    console.log('');
    console.log(chalk.yellow(`! Phase ${s.current.phase} is currently in progress (${s.current.status}).`));
    console.log(chalk.dim(`  Forcing phase ${phase}. State for phase ${s.current.phase} will be preserved.`));
  }

  // Warn if phase already completed
  if (s.phases_completed.includes(phase)) {
    console.log('');
    console.log(chalk.yellow(`! Phase ${phase} is already marked complete. Re-running.`));
    state.resetPhase(config, phase);
  }

  if (opts.builderModel)  config.models.builder  = opts.builderModel;
  if (opts.verifierModel) config.models.verifier = opts.verifierModel;

  try {
    await runPhase(config, phase, {
      builderModel:  config.models.builder,
      verifierModel: config.models.verifier,
      interactive:   opts.interactive,
    });
  } catch (e) {
    console.error(chalk.red(`\nFatal: ${e.message}`));
    if (process.env.ABL_DEBUG) console.error(e.stack);
    process.exit(1);
  }
}

module.exports = phaseCommand;
--- ./src/commands/init.js ---
'use strict';

const fs = require('fs');
const path = require('path');
const readline = require('readline');
const chalk = require('chalk');
const { execSync } = require('child_process');

const PACKAGE_ROOT = path.join(__dirname, '..', '..');
const TEMPLATE_DIR = path.join(PACKAGE_ROOT, '.abl');

async function initCommand() {
  const rl = readline.createInterface({ input: process.stdin, output: process.stdout });
  const ask = (q) => new Promise(res => rl.question(q, res));
  
  console.log(chalk.bold('\nABL Initialize'));
  const src = await ask('Source directory name? (default: src): ') || 'src';
  const tests = await ask('Tests directory name? (default: tests): ') || 'tests';
  rl.close();

  const cwd = process.cwd();
  const ablDir = path.join(cwd, '.abl');
  const promptsDir = path.join(ablDir, 'prompts'); // Inside .abl

  // 1. Create Directory Structure
  [
    ablDir, 
    path.join(ablDir, 'specs'), 
    path.join(ablDir, 'logs'), 
    promptsDir,
    path.join(cwd, src), 
    path.join(cwd, tests)
  ].forEach(d => {
    if (!fs.existsSync(d)) fs.mkdirSync(d, { recursive: true });
  });

  // 2. Copy Configuration Templates
  const copyTemplate = (file, dest = file) => {
    const s = path.join(TEMPLATE_DIR, file);
    if (fs.existsSync(s)) {
      let c = fs.readFileSync(s, 'utf8');
      if (file === 'abl.config.yaml') {
        c = c.replace(/src: .*/, `src: ./${src}`).replace(/tests: .*/, `tests: ./${tests}`);
      }
      fs.writeFileSync(path.join(ablDir, dest), c);
    }
  };
  ['abl.config.yaml', 'lean_settings.json', 'geminiignore.txt'].forEach(f => copyTemplate(f));

  // 3. Copy Role Prompts into .abl/prompts
  ['builder.md', 'verifier.md'].forEach(f => {
    const srcPrompt = path.join(PACKAGE_ROOT, 'prompts', f);
    const destPrompt = path.join(promptsDir, f);
    if (fs.existsSync(srcPrompt)) {
      fs.writeFileSync(destPrompt, fs.readFileSync(srcPrompt, 'utf8'));
    }
  });

  // 4. Initialize Base Files
  fs.writeFileSync(path.join(ablDir, 'project.md'), '# Project Name\n');
  fs.writeFileSync(path.join(ablDir, 'state.json'), JSON.stringify({ 
    phases_completed: [], 
    current: null, 
    phase_titles: {} 
  }, null, 2));
  fs.writeFileSync(path.join(ablDir, 'specs', 'phase1.md'), '# Phase 1: Setup\n\n### Contracts\n- Action -> Result\n');

  // 5. Initialize Git Repositories
  [src, tests].forEach(dirName => {
    const dirPath = path.join(cwd, dirName);
    try {
      execSync('git init --quiet', { cwd: dirPath });
      execSync('git config user.name "ABL"', { cwd: dirPath });
      execSync('git config user.email "abl@local"', { cwd: dirPath });
      execSync('git add -A', { cwd: dirPath });
      execSync('git commit -m "Initial commit" --allow-empty --quiet', { cwd: dirPath });
    } catch (e) {}
  });

  console.log(chalk.green('\n✓ ABL Initialized.'));
  console.log('\n  Available Commands:');
  console.log(chalk.dim('  abl run         Run or resume pending phase'));
  console.log(chalk.dim('  abl phase <N>   Run specific phase'));
  console.log(chalk.dim('  abl costs       Show token usage\n'));
}

module.exports = initCommand;
--- ./src/lib/tokens.js ---
'use strict';

const fs = require('fs');
const path = require('path');
const chalk = require('chalk');

const CSV_HEADER = 'timestamp,phase,step,role,model,input,candidates,cached,total,session_id\n';

/**
 * Nuclear fallback: Extracts the largest valid JSON object containing "stats" 
 * by tracking brace depth.
 */
function findStatsJson(str) {
  let startIdx = str.indexOf('{');
  while (startIdx !== -1) {
    let depth = 0;
    let inString = false;
    for (let i = startIdx; i < str.length; i++) {
      const char = str[i];
      // Handle strings to avoid miscounting braces inside quotes
      if (char === '"' && str[i - 1] !== '\\') inString = !inString;
      if (!inString) {
        if (char === '{') depth++;
        else if (char === '}') {
          depth--;
          if (depth === 0) {
            const candidate = str.substring(startIdx, i + 1);
            if (candidate.includes('"stats"')) {
              try {
                return JSON.parse(candidate);
              } catch (e) {
                // Not valid JSON, keep looking
              }
            }
          }
        }
      }
    }
    startIdx = str.indexOf('{', startIdx + 1);
  }
  return null;
}

function extractTokens(config, logPath, role, phase, step) {
  const { tokensCsv } = config.resolved;
  if (!fs.existsSync(tokensCsv)) fs.writeFileSync(tokensCsv, CSV_HEADER);
  
  let raw = '';
  try { raw = fs.readFileSync(logPath, 'utf8'); } catch (e) { return; }

  const ts = new Date().toISOString().replace('T', ' ').slice(0, 19);
  const parsed = findStatsJson(raw);
  const sessionId = parsed?.session_id || 'none';

  if (!parsed || !parsed.stats || !parsed.stats.models) {
    const errorRow = [`"${ts}"`,`"${phase}"`,`"${step}"`,`"${role}"`,'"unknown"','"ERROR"',0,0,0,`"${sessionId}"`].join(',');
    fs.appendFileSync(tokensCsv, errorRow + '\n');
    return;
  }

  try {
    for (const [model, data] of Object.entries(parsed.stats.models)) {
      const t = data.tokens || {};
      const row = [
        `"${ts}"`,
        `"${phase}"`,
        `"${step}"`,
        `"${role}"`,
        `"${model}"`,
        t.input || 0,
        t.candidates || 0,
        t.cached || 0,
        t.total || 0,
        `"${sessionId}"`
      ].join(',');
      fs.appendFileSync(tokensCsv, row + '\n');
    }
  } catch (e) {
    const errorRow = [`"${ts}"`,`"${phase}"`,`"${step}"`,`"${role}"`,'"unknown"','"ERROR"',0,0,0,`"${sessionId}"`].join(',');
    fs.appendFileSync(tokensCsv, errorRow + '\n');
  }
}

function printCosts(config) {
  const { tokensCsv } = config.resolved;
  if (!fs.existsSync(tokensCsv)) return console.log('No costs logged.');
  
  const lines = fs.readFileSync(tokensCsv, 'utf8').trim().split('\n').slice(1);
  let totals = { calls: 0, input: 0, output: 0, total: 0 };
  
  lines.forEach(l => {
    const c = l.split(',');
    // Handle quoted or unquoted ERROR strings
    if (c[5]?.includes('ERROR')) return;
    totals.calls++;
    totals.input += parseInt(c[5]) || 0;
    totals.output += parseInt(c[6]) || 0;
    totals.total += parseInt(c[8]) || 0;
  });

  console.log(chalk.bold(`\n  Total Calls: ${totals.calls}`));
  console.log(chalk.bold(`  Total Tokens: ${totals.total.toLocaleString()}\n`));
}

module.exports = { extractTokens, printCosts, findStatsJson };
--- ./src/lib/logger.js ---
'use strict';

const chalk = require('chalk');

const DIVIDER     = '━'.repeat(57);
const DIVIDER_SML = '─'.repeat(57);

module.exports = {
  phase(n, total) {
    console.log('');
    console.log(chalk.bold(`━━━ Verifier iteration ${n} / ${total} ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━`));
  },

  attempt(hi, maxH) {
    console.log(`  ${chalk.dim('──')} Build / health attempt ${hi} / ${maxH}`);
  },

  info(msg) {
    console.log(`  ${chalk.cyan('⚙')}  ${msg}`);
  },

  success(msg) {
    console.log(`  ${chalk.green('✓')}  ${msg}`);
  },

  fail(msg) {
    console.log(`  ${chalk.red('✗')}  ${msg}`);
  },

  warn(msg) {
    console.log(`  ${chalk.yellow('!')}  ${msg}`);
  },

  resume(items) {
    console.log('');
    console.log(chalk.yellow('↺  Resuming from prior state...'));
    for (const item of items) {
      console.log(`   ${chalk.dim('→')} ${item}`);
    }
  },

  stuck(reason, hint) {
    console.log('');
    console.log(chalk.red(DIVIDER));
    console.log(chalk.red(`✗  STUCK — ${reason}`));
    console.log(chalk.dim(`    → ${hint}`));
    console.log(chalk.red(DIVIDER));
  },

  passed(phase) {
    console.log('');
    console.log(chalk.green(DIVIDER));
    console.log(chalk.green(`✓  Phase ${phase} passed`));
    console.log(chalk.green(DIVIDER));
  },

  contractsFailed(testsDir) {
    console.log(`  ${chalk.red('✗')}  Contracts failed — see ${testsDir}/failed_specs.md`);
  },

  divider() {
    console.log(chalk.dim(DIVIDER_SML));
  },

  blank() {
    console.log('');
  }
};
--- ./src/lib/map.js ---
'use strict';

const { execSync } = require('child_process');
const fs = require('fs');

function generateMap(config) {
  const { srcDir, projectMap } = config.resolved;
  
  try {
    // Generate file tree
    let map = execSync(`tree "${srcDir}" -I 'node_modules|.git' --dirsfirst`, { encoding: 'utf8' });
    
    // Append dependency info if available
    const pkgPath = require('path').join(srcDir, 'package.json');
    if (fs.existsSync(pkgPath)) {
      map += '\n--- Dependencies ---\n';
      map += fs.readFileSync(pkgPath, 'utf8');
    }

    fs.writeFileSync(projectMap, map);
  } catch (e) {
    fs.writeFileSync(projectMap, `Error generating map: ${e.message}`);
  }
}

module.exports = { generateMap };
--- ./src/lib/context.js ---
'use strict';

const fs = require('fs');
const path = require('path');
const stateLib = require('./state');
const { buildCommandContext } = require('./config');

function buildPipedContext(config, role, phase, iteration) {
  const state = stateLib.read(config);
  
  const projectMd = fs.existsSync(path.join(config.resolved.ablDir, 'project.md'))
    ? fs.readFileSync(path.join(config.resolved.ablDir, 'project.md'), 'utf8')
    : '# Project\nNo description provided.';

  const projectMap = fs.existsSync(config.resolved.projectMap)
    ? fs.readFileSync(config.resolved.projectMap, 'utf8')
    : 'No project map available.';

  const rolePromptPath = path.join(config.resolved.promptsDir, `${role}.md`);
  const rolePrompt = fs.existsSync(rolePromptPath)
    ? fs.readFileSync(rolePromptPath, 'utf8')
    : `You are the ${role}. Implement the requested phase.`;

  const currentSpecPath = path.join(config.resolved.specsDir, `phase${phase}.md`);
  const currentSpec = fs.existsSync(currentSpecPath)
    ? fs.readFileSync(currentSpecPath, 'utf8')
    : 'No spec found for this phase.';

  // --- Failure Context for Builder ---
  let failureContext = '';
  if (role === 'builder') {
    const failPath = path.join(config.resolved.testsDir, 'failed_specs.md');
    if (fs.existsSync(failPath) && fs.readFileSync(failPath, 'utf8').trim().length > 0) {
      failureContext = [
        '# PREVIOUS ATTEMPT FAILED',
        'Your previous implementation did not satisfy all contracts.',
        'Address the following failures in your next implementation:',
        '',
        fs.readFileSync(failPath, 'utf8'),
        ''
      ].join('\n');
    } else {
      failureContext = '# INITIAL BUILD\nThis is a fresh iteration. No prior failures detected.';
    }
  }

  const historyLines = Object.entries(state.phase_titles)
    .sort((a, b) => parseInt(a[0]) - parseInt(b[0]))
    .map(([num, title]) => {
      const status = state.phases_completed.includes(parseInt(num)) ? '[COMPLETED]' : '[PENDING]';
      return `- Phase ${num}: ${title} ${status}`;
    });

  const cmdContext = buildCommandContext(config, role);

  return [
    rolePrompt,
    '',
    '# Project Context',
    projectMd,
    '',
    '# System Topology',
    projectMap,
    '',
    '# Session State',
    `Current Phase: ${phase} (${state.phase_titles[phase] || 'Unknown'})`,
    `Current Iteration: ${iteration}`,
    '',
    failureContext,
    '',
    '# Phase History',
    historyLines.join('\n'),
    '',
    '# Current Phase Spec (CONTRACTS)',
    currentSpec,
    '',
    '# Available Commands',
    cmdContext
  ].join('\n');
}

module.exports = { buildPipedContext };
--- ./src/lib/executor.js ---
'use strict';

const { spawnSync, execSync } = require('child_process');
const fs = require('fs');
const path = require('path');
const { buildPipedContext } = require('./context');
const { generateMap } = require('./map');
const os = require('os');
const chalk = require('chalk');

/**
 * Creates a temporary abl-cmd executable script inside the workspace
 * to facilitate local execution without a proxy server.
 */
function setupAblCmd(config, role, workspace) {
  const commands = role === 'builder' ? config.builder_commands : config.verifier_commands;
  const specsDir = config.resolved.specsDir;

  let scriptContent = `#!/usr/bin/env node
const { execSync } = require('child_process');
const fs = require('fs');
const path = require('path');

const cmd = process.argv[2];
const args = process.argv.slice(3);

if (cmd === 'get-spec') {
  const phase = args[0];
  const p = path.join('${specsDir.replace(/\\/g, '\\\\')}', \`phase\${phase}.md\`);
  if (fs.existsSync(p)) {
    console.log(fs.readFileSync(p, 'utf8'));
    process.exit(0);
  } else {
    console.error('Spec not found');
    process.exit(1);
  }
}

const commands = ${JSON.stringify(commands || {})};
if (commands[cmd]) {
  try {
    const out = execSync(commands[cmd].command, { stdio: 'inherit', encoding: 'utf8' });
    process.exit(0);
  } catch (e) {
    process.exit(1);
  }
}

console.error('Unknown command: ' + cmd);
process.exit(1);
`;

  const binDir = path.join(os.tmpdir(), `abl_bin_${role}_${Date.now()}`);
  if (!fs.existsSync(binDir)) fs.mkdirSync(binDir, { recursive: true });
  
  const cmdPath = path.join(binDir, 'abl-cmd');
  fs.writeFileSync(cmdPath, scriptContent, { mode: 0o755 });
  return binDir;
}

async function runRole(role, config, phase, opts) {
  const workspace = role === 'builder' ? config.resolved.srcDir : config.resolved.testsDir;
  const iteration = opts.iteration || 1;
  const logPath = path.join(config.resolved.logsDir, `${role}.log`);
  const isInteractive = !!opts.interactive;

  generateMap(config);
  const fullPrompt = buildPipedContext(config, role, phase, iteration);
  const binDir = setupAblCmd(config, role, workspace);

  const env = {
    ...process.env,
    PATH: `${binDir}${path.delimiter}${process.env.PATH}`,
    GEMINI_CLI_SYSTEM_SETTINGS_PATH: path.join(config.resolved.ablDir, 'lean_settings.json'),
    GEMINI_API_KEY: process.env.GEMINI_API_KEY
  };

  const model = opts.model || (role === 'builder' ? config.models.builder : config.models.verifier);
  
  const args = [
    '-m', model,
    '-y',
    '--output-format', 'json'
  ];

  if (isInteractive) {
    args.push('-i', fullPrompt);
    console.log(chalk.blue(`\n[Entering Interactive Mode for ${role}]`));
    console.log(chalk.dim(`Type '/quit' to finish the ${role} turn and continue the loop.\n`));
    
    // In interactive mode, we must use 'inherit' to allow user input/output
    spawnSync('gemini', args, {
      cwd: workspace,
      env,
      stdio: 'inherit'
    });

    return logPath; // Log will be empty in this mode
  } else {
    args.push('-p', fullPrompt);
    const result = spawnSync('gemini', args, {
      cwd: workspace,
      env,
      encoding: 'utf8',
      maxBuffer: 10 * 1024 * 1024
    });

    const output = (result.stdout || '') + (result.stderr || '');
    fs.writeFileSync(logPath, output);

    if (result.status !== 0) {
      throw new Error(`${role} failed with exit code ${result.status}. See logs.`);
    }

    return logPath;
  }
}

module.exports = {
  runBuilder: (config, phase, opts) => runRole('builder', config, phase, opts),
  runVerifier: (config, phase, opts) => runRole('verifier', config, phase, opts)
};
--- ./src/lib/config.js ---
'use strict';

const fs = require('fs');
const path = require('path');
const yaml = require('js-yaml');

const DEFAULTS = {
  models: { builder: 'gemini-2.0-flash', verifier: 'gemini-2.0-flash' },
  loop: { max_verifier_iterations: 5 }
};

function loadConfig(cwd = process.cwd()) {
  const ablDir = path.join(cwd, '.abl');
  const configFile = path.join(ablDir, 'abl.config.yaml');
  if (!fs.existsSync(configFile)) throw new Error('No ABL project found. Run "abl init".');
  
  const raw = yaml.load(fs.readFileSync(configFile, 'utf8')) || {};
  const dirs = raw.directories || {};

  return {
    ...DEFAULTS,
    ...raw,
    models: raw.models ? { ...DEFAULTS.models, ...raw.models } : DEFAULTS.models,
    loop: { ...DEFAULTS.loop, ...(raw.loop || {}) },
    resolved: {
      root: cwd,
      ablDir,
      specsDir: path.join(ablDir, 'specs'),
      logsDir: path.join(ablDir, 'logs'),
      promptsDir: path.join(ablDir, 'prompts'),
      tokensCsv: path.join(ablDir, 'logs', 'tokens.csv'),
      projectMap: path.join(ablDir, 'project_map.txt'),
      stateFile: path.join(ablDir, 'state.json'),
      srcDir: path.resolve(cwd, dirs.src || './src'),
      testsDir: path.resolve(cwd, dirs.tests || './tests')
    }
  };
}

function buildCommandContext(config, role) {
  const commands = role === 'builder' ? config.builder_commands : config.verifier_commands;
  const lines = [
    'You have access to "abl-cmd" via the system shell.',
    'Usage: abl-cmd <name> [args]',
    '- abl-cmd get-spec <N> : Returns the content of phaseN.md spec'
  ];
  if (commands) {
    Object.entries(commands).forEach(([name, def]) => {
      lines.push(`- abl-cmd ${name} : ${def.description || name}`);
    });
  }
  return lines.join('\n');
}

module.exports = { loadConfig, buildCommandContext };
--- ./src/lib/state.js ---
'use strict';

const fs = require('fs');
const path = require('path');
const { execSync } = require('child_process');

const STATUS = { 
  CLEAN: 'clean', 
  BUILDING: 'building', 
  VERIFYING: 'verifying', 
  CONTRACTS_FAILED: 'contracts_failed', 
  STUCK: 'stuck', 
  COMPLETED: 'completed' 
};

function read(config) {
  const p = config.resolved.stateFile;
  let state = { phases_completed: [], current: null, last_updated: null, phase_titles: {} };
  
  if (fs.existsSync(p)) {
    try { state = JSON.parse(fs.readFileSync(p, 'utf8')); } catch (e) {}
  }

  // Sync titles from filesystem (force integers for keys)
  const specsDir = config.resolved.specsDir;
  if (fs.existsSync(specsDir)) {
    fs.readdirSync(specsDir).filter(f => f.match(/^phase(\d+)\.md$/)).forEach(f => {
      const n = parseInt(f.match(/\d+/)[0]);
      const content = fs.readFileSync(path.join(specsDir, f), 'utf8');
      state.phase_titles[n] = content.split('\n')[0].replace(/^#\s*/, '').trim();
    });
  }
  return state;
}

function write(config, state) {
  state.last_updated = new Date().toISOString();
  fs.writeFileSync(config.resolved.stateFile, JSON.stringify(state, null, 2));
}

function syncWithGit(config, state) {
  const gitDir = path.join(config.resolved.srcDir, '.git');
  if (!fs.existsSync(gitDir)) return state;

  try {
    const log = execSync('git log --oneline', { cwd: config.resolved.srcDir, encoding: 'utf8', stdio: ['ignore', 'pipe', 'ignore'] });
    const matches = [...log.matchAll(/phase(\d+)\/passed/g)];
    
    matches.forEach(m => {
      const phaseNum = parseInt(m[1]);
      if (!state.phases_completed.includes(phaseNum)) {
        state.phases_completed.push(phaseNum);
      }
    });
    
    state.phases_completed.sort((a, b) => a - b);
  } catch (e) {}
  return state;
}

function resolveNextAction(config, state) {
  if (!fs.existsSync(config.resolved.specsDir)) return { action: 'done', phase: null, reason: 'no_specs' };
  
  const specs = fs.readdirSync(config.resolved.specsDir)
    .filter(f => f.match(/^phase(\d+)\.md$/))
    .map(f => parseInt(f.match(/\d+/)[0]))
    .sort((a, b) => a - b);

  if (specs.length === 0) return { action: 'done', phase: null, reason: 'no_specs' };
  
  if (state.current && state.current.status !== STATUS.COMPLETED && state.current.status !== STATUS.STUCK) {
    return { action: 'resume', phase: state.current.phase, status: state.current.status };
  }

  const next = specs.find(n => !state.phases_completed.includes(n));
  return next ? { action: 'run', phase: next } : { action: 'done', phase: null, reason: 'all_complete' };
}

module.exports = { 
  STATUS, 
  read, 
  write, 
  syncWithGit, 
  resolveNextAction, 
  startPhase: (config, phase) => { 
    const s = read(config); 
    s.current = { 
      phase: parseInt(phase), 
      verifier_iteration: 1, 
      status: STATUS.CLEAN,
      builder_success: false,
      verifier_success: false
    }; 
    write(config, s); 
  },
  updateProgress: (config, updates) => { 
    const s = read(config); 
    if (s.current) { 
      if (updates.verifierIteration) s.current.verifier_iteration = updates.verifierIteration; 
      if (updates.status) s.current.status = updates.status; 
      if (updates.builder_success !== undefined) s.current.builder_success = updates.builder_success;
      if (updates.verifier_success !== undefined) s.current.verifier_success = updates.verifier_success;
      write(config, s); 
    } 
  },
  completePhase: (config, phase) => { 
    const s = read(config); 
    const p = parseInt(phase);
    if (!s.phases_completed.includes(p)) s.phases_completed.push(p); 
    s.current = null; 
    write(config, s); 
  },
  resetPhase: (config, phase) => { 
    const s = read(config); 
    const p = parseInt(phase);
    s.phases_completed = s.phases_completed.filter(n => n !== p); 
    s.current = null; 
    write(config, s); 
  }
};
--- ./src/lib/loop.js ---
'use strict';

const log = require('./logger');
const git = require('./git');
const executor = require('./executor');
const tokens = require('./tokens');
const state = require('./state');
const fs = require('fs');
const path = require('path');

async function runPhase(config, phase, opts = {}) {
  const maxVi = config.loop.max_verifier_iterations;
  const bModel = opts.builderModel || config.models.builder;
  const vModel = opts.verifierModel || config.models.verifier;
  const isInteractive = !!opts.interactive;

  let s = state.read(config);
  if (!s.current || s.current.phase !== phase) {
    state.startPhase(config, phase);
    s = state.read(config);
  }

  git.ensureRepo(config.resolved.srcDir);
  git.ensureRepo(config.resolved.testsDir);

  for (let vi = s.current.verifier_iteration; vi <= maxVi; vi++) {
    log.phase(vi, maxVi);
    s = state.read(config); // Refresh state each iteration

    // --- BUILDER TURN ---
    if (!s.current.builder_success) {
      state.updateProgress(config, { verifierIteration: vi, status: state.STATUS.BUILDING });
      log.info('Builder running...');
      try {
        const bLog = await executor.runBuilder(config, phase, { 
          iteration: vi, 
          model: bModel, 
          interactive: isInteractive 
        });
        tokens.extractTokens(config, bLog, 'builder', phase, vi);
        git.commitSrc(config, phase, vi, 'build');
        state.updateProgress(config, { builder_success: true });
        log.success('Builder turn complete');
      } catch (e) {
        log.fail(`Builder crashed: ${e.message}`);
        state.updateProgress(config, { status: state.STATUS.STUCK });
        return;
      }
    } else {
      log.info('Builder step already completed. Skipping...');
    }

    // --- VERIFIER TURN ---
    let vLogPath = path.join(config.resolved.logsDir, 'verifier.log');
    if (!s.current.verifier_success) {
      state.updateProgress(config, { status: state.STATUS.VERIFYING });
      log.info('Verifier running...');
      try {
        const vLog = await executor.runVerifier(config, phase, { 
          iteration: vi, 
          model: vModel, 
          interactive: isInteractive // Add this
        });
        tokens.extractTokens(config, vLog, 'verifier', phase, vi);
        git.commitTests(config, phase, vi);
        state.updateProgress(config, { verifier_success: true });
        log.success('Verifier turn complete');
      } catch (e) {
        log.fail(`Verifier crashed: ${e.message}`);
        state.updateProgress(config, { status: state.STATUS.STUCK });
        return;
      }
    } else {
      log.info('Verifier step already completed. Skipping...');
    }

    // --- EVALUATE ITERATION ---
    const vContent = fs.readFileSync(vLogPath, 'utf8');
    const vParsed = tokens.findStatsJson(vContent);
    const verifierTaskSuccess = vParsed && vParsed.stats && !vContent.includes('Error executing tool');
    
    const failPath = path.join(config.resolved.testsDir, 'failed_specs.md');
    const hasContractFailures = fs.existsSync(failPath) && fs.readFileSync(failPath, 'utf8').trim().includes('SPEC:');

    if (verifierTaskSuccess && !hasContractFailures) {
      git.commitPass(config, phase);
      state.completePhase(config, phase);
      log.passed(phase);
      return;
    }

    if (!verifierTaskSuccess) {
      log.fail('Verifier iteration failed to execute correctly. Check logs.');
      state.updateProgress(config, { status: state.STATUS.STUCK });
      return;
    }

    // Prepare for next iteration
    log.contractsFailed(config.resolved.testsDir);
    state.updateProgress(config, { 
      status: state.STATUS.CONTRACTS_FAILED,
      builder_success: false, 
      verifier_success: false,
      verifierIteration: vi + 1
    });
  }

  log.stuck('Max iterations reached.', 'Refine your specs.');
  state.updateProgress(config, { status: state.STATUS.STUCK });
}

module.exports = { runPhase };
--- ./src/lib/git.js ---
'use strict';

const { execSync } = require('child_process');
const fs = require('fs');
const path = require('path');

function ensureRepo(dir) {
  if (!fs.existsSync(path.join(dir, '.git'))) {
    execSync('git init --quiet', { cwd: dir });
    try {
      execSync('git config user.name "ABL"', { cwd: dir });
      execSync('git config user.email "abl@local"', { cwd: dir });
    } catch (e) {}
  }
}

function commit(dir, message, allowEmpty = false) {
  try {
    execSync('git add -A', { cwd: dir, stdio: 'pipe' });
    const diff = execSync('git diff --cached --name-only', { cwd: dir, encoding: 'utf8' });
    if (!diff.trim() && !allowEmpty) return;
    execSync(`git commit -m "${message}" ${allowEmpty ? '--allow-empty' : ''} --quiet`, { cwd: dir });
  } catch (e) {}
}

module.exports = {
  commitSrc: (config, phase, step, suffix) => commit(config.resolved.srcDir, `phase${phase}/build/step-${step}/${suffix}`),
  commitTests: (config, phase, step) => commit(config.resolved.testsDir, `phase${phase}/verify/step-${step}/results`),
  commitPass: (config, phase) => commit(config.resolved.srcDir, `phase${phase}/passed`, true),
  ensureRepo
};
--- ./examples/nextjs/abl.config.yaml ---
directories:
  src: ./src
  tests: ./tests

models:
  builder: gemini-2.5-pro
  verifier: gemini-2.5-flash

commands:
  health_check: npm run lint && npx tsc --noEmit
  start_dev: npm run dev
  reset_state: npm run db:seed
  map_deps: cat package.json

dev_server:
  port: 3000
  ready_endpoint: /api/health
  timeout_ms: 15000

loop:
  max_health_attempts: 10
  max_verifier_iterations: 5

--- ./examples/nextjs/abl.config.sh ---
#!/bin/bash
# abl.config.sh — project-specific configuration for ABL
# Edit this file per project. Never edit the Makefile.

COMMAND=${1}

case "$COMMAND" in

  # ── Path configuration ────────────────────────────────────────────────────
  # Return the source code directory (relative to project root)
  src_dir)
    echo "src"
    ;;

  # Return the tests directory (relative to project root)
  tests_dir)
    echo "tests"
    ;;

  # ── Dev server ────────────────────────────────────────────────────────────
  start_dev)
    mkdir -p logs
    kill $(lsof -ti:3000) 2>/dev/null
    rm -f src/.next/dev/lock
    sleep 1
    cd src
    npm run dev > ../logs/dev.log 2>&1 &
    echo $! > ../logs/dev.pid
    cd ..
    sleep 5
    ;;

  stop_dev)
    kill -- -$(cat logs/dev.pid) 2>/dev/null
    pkill -f "next dev" 2>/dev/null
    kill $(lsof -ti:3000) 2>/dev/null
    rm -f src/.next/dev/lock
    ;;

  # ── Deterministic health checks ───────────────────────────────────────────
  # All output captured to logs/health.log
  # Return non-zero if any check fails
  health_check)
    mkdir -p logs
    > logs/health.log

    cd src

    echo "=== LINT ===" >> ../logs/health.log
    npm run lint >> ../logs/health.log 2>&1
    lint_exit=$?

    echo "=== TYPECHECK ===" >> ../logs/health.log
    npx tsc --noEmit >> ../logs/health.log 2>&1
    tsc_exit=$?

    # Add more checks as needed:
    # echo "=== UNIT TESTS ===" >> ../logs/health.log
    # npm test -- --passWithNoTests >> ../logs/health.log 2>&1
    # test_exit=$?

    cd ..
    [ $lint_exit -eq 0 ] && [ $tsc_exit -eq 0 ]
    ;;

  # ── State reset ───────────────────────────────────────────────────────────
  # Runs before every Verifier iteration
  # No-op if stateless, otherwise reset DB/cache/queues
  reset_state)
    cd src
    npm run db:seed || { echo "  ✗  Seed failed — aborting"; cd ..; exit 1; }
    cd ..
    ;;

  # ── Dependency map ────────────────────────────────────────────────────────
  # Appended to project_map.txt before every LLM call
  map_deps)
    cat src/package.json
    ;;

  *)
    echo "Unknown command: $COMMAND"
    exit 1
    ;;

esac